\section{Methodology}

%======================================================================================
\subsection{Classical equivalent layer for magnetic data}
%======================================================================================

Let $\mathbf{d}^{o}$ be the $N \times 1$ observed data vector, whose $i$th element 
is the total-field anomaly $d^{o}_{i}$ produced by arbitrarily magnetized sources
at the position $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $, 
of a right-handed Cartesian coordinate system with $x$-, $y$- and $z$-axis 
pointing to north, east and down, respectively.
We consider that the total-field anomaly data $d^{o}_{i}$ represent the discrete
values of a harmonic function. Besides, we consider that the main geomagnetic field 
direction at the study area can be defined by the unit vector
\begin{equation}
\hat{\mathbf{F}} = \begin{bmatrix}
F_x \\
F_y \\
F_z
\end{bmatrix} =
\begin{bmatrix}
\cos(I_{0}) \, \cos(D_{0}) \\
\cos(I_{0}) \, \sin(D_{0}) \\
\sin(I_{0})
\end{bmatrix} \: ,
\label{eq:unit_vector_F}
\end{equation}
with constant inclination $I_{0}$ and declination $D_{0}$.
In this case, $d^{o}_{i}$ can be approximated by the predicted total-field anomaly
\begin{equation}
\Delta T_{i} = \sum_{j=1}^{M} \, p_{j} a_{ij} \: ,
\label{eq:integral-sum_mag}
\end{equation}
which describes the magnetic induction exerted, at the observation point $(x_{i}, y_{i}, z_{i})$,
by a discrete layer of $M$ dipoles (equivalent sources) defined on the horizontal plane $z = z_{c}$, 
where $p_{j}$ is the magnetic moment intensity (in A~m~$^{2}$)~of the $j$th dipole, 
that has unit volume and is located at the point $(x_{j}, y_{j}, z_{c})$. In equation
\ref{eq:integral-sum_mag}, $a_{ij}$ is the harmonic function
\begin{equation}
a_{ij}
= c_{m} \, \frac{\mu_{0}}{4\pi} \, \hat{\mathbf{F}}^{\top} \mathbf{H}_{ij} \: \hat{\mathbf{u}} \: ,
\label{eq:aij_mag}
\end{equation}
the unit vector
\begin{equation}
\hat{\mathbf{u}} = \begin{bmatrix}
u_x \\
u_y \\
u_z
\end{bmatrix} =
\begin{bmatrix}
\cos(I) \, \cos(D) \\
\cos(I) \, \sin(D) \\
\sin(I)
\end{bmatrix} \: ,
\label{eq:u_hat}
\end{equation}
defines the magnetization direction of all dipoles, with constant inclination $I$ and declination $D$,
$\mu_{0} = 4\pi \, 10^{-7}$ H/m is the magnetic constant, $c_{m} = 10^{9}$ is a factor that transforms
the magnetic induction from Tesla (T) to nanotesla (nT) and $\mathbf{H}_{ij}$ is a $3 \times 3$ matrix 
\begin{equation}
\mathbf{H}_{ij} = \begin{bmatrix}
h^{xx}_{ij} & h^{xy}_{ij} & h^{xz}_{ij} \\
h^{xy}_{ij} & h^{yy}_{ij} & h^{yz}_{ij} \\
h^{xz}_{ij} & h^{yz}_{ij} & h^{zz}_{ij}
\end{bmatrix} \: ,
\label{eq:Hij}
\end{equation}
where 
\begin{equation}
h^{\alpha\beta}_{ij} = 
\begin{cases}
\frac{3 \left( \alpha_{i} - \alpha_{j} \right)^{2}}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: , \quad \alpha = \beta \\
\frac{3 \left( \alpha_{i} - \alpha_{j} \right) \left( \beta_{i} - \beta_{j} \right)}{r_{ij}^{5}} \: , \quad \alpha \ne \beta
\end{cases} \: , \quad \alpha, \beta = x, y, z \: ,
\label{eq:hij_alpha_beta}
\end{equation}
are the second derivatives of the inverse distance function
\begin{equation}
\frac{1}{r_{ij}} = 
\frac{1}{\sqrt{\left(x_{i} - x_{j} \right)^{2} + 
\left(y_{i} - y_{j} \right)^{2} + \left(z_{i} - z_{c} \right)^{2}}}
\label{eq:1_rij}
\end{equation}
with respect to the coordinates of the observation point $(x_{i}, y_{i}, z_{i})$.

Equation \ref{eq:integral-sum_mag} can be rewritten in matrix notation as follows:
\begin{equation}
\mathbf{d}(\mathbf{p}) = \mathbf{A} \mathbf{p} \: ,
\label{eq:predicted-data-vector_mag}
\end{equation}
where $\mathbf{d}(\mathbf{p})$ is the $N \times 1$ predicted data vector with $i$th element defined
be the predicted total-field anomaly $\Delta T_{i}$ (equation \ref{eq:integral-sum_mag}),
$\mathbf{p}$ is the $M \times 1$ parameter vector whose $j$th element is the magnetic moment intensity
$p_{j}$ of the $j$th dipole and $\mathbf{A}$ is the $N \times M$ sensitivity matrix with element 
$ij$ defined by the harmonic function $a_{ij}$ (equation \ref{eq:aij_mag}).
In the classical equivalent-layer technique, the common approach for 
estimating the parameter vector $\mathbf{p}$ from the observed 
total-field anomaly data $\mathbf{d}^{o}$ is solving the least-squares normal equations
\begin{equation}
\mathbf{A}^{\top}\mathbf{A} \: \mathbf{p} = 
\mathbf{A}^{\top} \mathbf{d}^{o} \: .
\label{eq:normal-equations}
\end{equation}
Equation \ref{eq:normal-equations} is usually solved by first computing the Cholesky 
factor of matrix $\mathbf{A}^{\top}\mathbf{A}$ and then using it to solve the linear 
system \citep[][ p. 262]{golub-vanloan2013}. 
This approach to estimate the parameter vector will be 
referenced throughout this work as the \textit{classical method}.

The computational cost required for estimating the classical solution can be very high
when dealing with large datasets. In the following subsections, we will show how to 
explore the structure of the sensitivity matrix $\mathbf{A}$ and 
efficiently solve the least-squares normal equations (equation \ref{eq:normal-equations}).


%=====================================================================================================
\subsection{Matrix $\mathbf{A}$ in terms of matrix components $\mathbf{A_{\boldsymbol{\alpha\beta}}}$}
%=====================================================================================================

To access the structure of the sensitivity matrix $\mathbf{A}$ 
(equation \ref{eq:predicted-data-vector_mag}), let us first rewrite its elements 
$a_{ij}$ (equation \ref{eq:aij_mag}) in the following way:
\begin{equation}
\begin{split}
a_{ij} = a^{xx}_{ij} + a^{xy}_{ij} + a^{xz}_{ij} + a^{yy}_{ij} + a^{yz}_{ij} + a^{zz}_{ij} \: ,
\end{split}
\label{eq:aij_mag_expand}
\end{equation}
where
\begin{equation}
a^{\alpha\beta}_{ij} = 
\begin{cases}
c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{\alpha} u_{\beta} \right) h^{\alpha\beta}_{ij} \: &, \quad \alpha = \beta \\
c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{\alpha} u_{\beta} + F_{\beta} u_{\alpha} \right) h^{\alpha\beta}_{ij} \: &, \quad \alpha \ne \beta \\
\end{cases}
\: , \quad \alpha, \beta = x, y, z \: ,
\label{eq:aij_alpha_beta}
\end{equation}
are defined by the elements of $\hat{\mathbf{F}}$ 
(equation \ref{eq:unit_vector_F}), $\hat{\mathbf{u}}$ (equation \ref{eq:u_hat}) and 
$\mathbf{H}_{ij}$ (equations \ref{eq:Hij} and \ref{eq:hij_alpha_beta}).
Then, we can rewrite the sensitivity matrix $\mathbf{A}$ 
(equation \ref{eq:predicted-data-vector_mag}) according to:
\begin{equation}
\mathbf{A} = \mathbf{A_{xx}} + \mathbf{A_{xy}} + \mathbf{A_{xz}} + 
\mathbf{A_{yy}} + \mathbf{A_{yz}} + \mathbf{A_{zz}} \: ,
\label{eq:A_expand}
\end{equation}
where $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ are $N \times M$ matrices with elements 
$ij$ defined by $a^{\alpha\beta}_{ij}$ (equation \ref{eq:aij_alpha_beta}).

Now we can define the structure of $\mathbf{A}$ in terms of its components 
$\mathbf{A_{\boldsymbol{\alpha\beta}}}$ (equation \ref{eq:A_expand}). To do this, 
we consider the particular case in which the observed total-field anomaly is located 
on an $N_{x} \times N_{y}$ 
regular grid of points spaced by $\Delta_{x}$ and $\Delta_{y}$ along the $x$- and $y$-directions,
respectively, on a constant vertical coordinate $z_{0}$. We also consider that the equivalent layer
is formed by one dipole right below each observation point, at the constant coordinate $z_{c}$.
In this case, the number of equivalent sources $M$ is equal to the number of data $N$ and, 
consequently, matrices $\mathbf{A}$ and $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ become 
square ($N \times N$). 
Besides, the horizontal coordinates $x_{i}$ and $y_{i}$ of the observation points 
can be defined by
\begin{equation}
x_{i} = x_{1} + \left[ k(i) - 1 \right] \, \Delta_{x}
\label{eq:xi}
\end{equation}
and
\begin{equation}
y_{i} = y_{1} + \left[ l(i) - 1 \right] \, \Delta_{y} \: ,
\label{eq:yi}
\end{equation}
where $x_{1}$ and $y_{1}$ are the lower limits for $x_{i}$ and $y_{i}$, respectively,
and $k(i)$ and $l(i)$ are integer functions defined according to the orientation
of the data grid (Figure \ref{fig:regular-grids}). 
For $x$-\textit{oriented grids}, the integer functions are given by
\begin{equation}
k(i)  = i - \Bigg\lceil \frac{i}{N_{x}} \Bigg\rceil N_{x} + N_{x}
\label{eq:k-x-oriented}
\end{equation}
and
\begin{equation}
l(i) = \Bigg\lceil \frac{i}{N_{x}} \Bigg\rceil \: .
\label{eq:l-x-oriented}
\end{equation}
For $y$-\textit{oriented grids}, the integer functions are given by
\begin{equation}
k(i) = \Bigg\lceil \frac{i}{N_{y}} \Bigg\rceil
\label{eq:k-y-oriented}
\end{equation}
and
\begin{equation}
l(i) = i - \Bigg\lceil \frac{i}{N_{y}} \Bigg\rceil N_{y} + N_{y} \: .
\label{eq:l-y-oriented}
\end{equation}
In equations \ref{eq:k-x-oriented}--\ref{eq:l-y-oriented}, $\lceil \cdot \rceil$ denotes the ceiling 
function \citep[e.g.,][ p. 67-68]{graham-etal1994}.
Equations \ref{eq:xi}--\ref{eq:l-y-oriented} can also be used to define the coordinates 
$x_{j}$ and $y_{j}$ of the equivalent sources, but with index $j$ instead of $i$.

By using equations \ref{eq:xi}--\ref{eq:l-y-oriented} to define the coordinates $x_{i}$ and 
$y_{i}$ of the observation points and $x_{j}$ and $y_{j}$ of the equivalent sources, we can
rewrite the elements $h^{\alpha\beta}_{ij}$ (equation \ref{eq:hij_alpha_beta}) of matrix 
$\mathbf{H}_{ij}$ (equation \ref{eq:Hij}) as follows:
\begin{equation}
h^{xx}_{ij} = 
\frac{3 \left( \Delta k_{ij} \, \Delta_{x} \right)^{2}}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: ,
\label{eq:hxx_regular}
\end{equation}
\begin{equation}
h^{yy}_{ij} = 
\frac{3 \left( \Delta l_{ij} \, \Delta_{y} \right)^{2}}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: ,
\label{eq:hyy_regular}
\end{equation}
\begin{equation}
h^{zz}_{ij} = 
\frac{3 \Delta_{z}^{2}}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: ,
\label{eq:hzz_regular}
\end{equation}
\begin{equation}
h^{xy}_{ij} = 
\frac{3 \left( \Delta k_{ij} \, \Delta_{x} \right)\left( \Delta l_{ij} \, \Delta_{y} \right)}{r_{ij}^{5}} \: ,
\label{eq:hxy_regular}
\end{equation}
\begin{equation}
h^{xz}_{ij} = 
\frac{3 \left( \Delta k_{ij} \, \Delta_{x} \right) \Delta_{z}}{r_{ij}^{5}}
\label{eq:hxz_regular}
\end{equation}
and
\begin{equation}
h^{yz}_{ij} = 
\frac{3 \left( \Delta l_{ij} \, \Delta_{y} \right) \Delta_{z}}{r_{ij}^{5}} \: ,
\label{eq:hyz_regular}
\end{equation}
where $\Delta_{z} = z_{c} - z_{0}$, 
\begin{equation}
\Delta k_{ij} = \frac{x_{i} - x_{j}}{\Delta_{x}} = k(i) - k(j) \: ,
\label{eq:Delta_kij}
\end{equation}
\begin{equation}
\Delta l_{ij} = \frac{y_{i} - y_{j}}{\Delta_{y}} = l(i) - l(j)
\label{eq:Delta_lij}
\end{equation}
and 
\begin{equation}
\frac{1}{r_{ij}} = 
\frac{1}{\sqrt{\left( \Delta k_{ij} \, \Delta_{x} \right)^{2} + \left( \Delta l_{ij} \, \Delta_{y} \right)^{2} + \Delta_{z}^{2}}} \: .
\label{eq:1_rij_regular}
\end{equation}
Note that the integer functions $k(i)$, $k(j)$, $l(i)$ and $l(j)$ (equations 
\ref{eq:k-x-oriented}--\ref{eq:l-y-oriented}) defining $\Delta k_{ij}$ (equation
\ref{eq:Delta_kij}), $\Delta l_{ij}$ (equation \ref{eq:Delta_lij}) and 
$\tfrac{1}{r_{ij}}$ (equation \ref{eq:1_rij_regular}) assume different 
forms depending on the grid orientation.
Despite of that, it can be shown that
\begin{equation}
\Delta k_{ij} = - \Delta k_{ji} \: ,
\label{eq:Delta_kij_symmetry}
\end{equation}
\begin{equation}
\Delta l_{ij} = - \Delta l_{ji}
\label{eq:Delta_lij_symmetry}
\end{equation}
and 
\begin{equation}
\frac{1}{r_{ij}} = \frac{1}{r_{ji}}
\label{eq:1_rij_symmetry}
\end{equation}
for any grid orientation.

%=================================================================================
\subsection{General structure of matrices $\mathbf{A_{\boldsymbol{\alpha\beta}}}$}
%=================================================================================

By using equations \ref{eq:hxx_regular}--\ref{eq:1_rij_regular} to compute 
$a^{\alpha\beta}_{ij}$ (equation \ref{eq:aij_alpha_beta}), we can show that 
matrices $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ (equation \ref{eq:A_expand}) assume
well-defined structures that can be conveniently
represented with \textit{block indices} $q$ and $p$ \citep{takahashi2020convolutional}.
These indices are defined by the integer functions $\Delta k_{ij}$ and $\Delta l_{ij}$ 
(equations \ref{eq:Delta_kij} and \ref{eq:Delta_lij}), in terms of the indices $i$ 
of the observation points $(x_{i}, y_{i}, z_{0})$ and $j$ of the equivalent sources
$(x_{j}, y_{j}, z_{c})$.
For $x$-\textit{oriented grids} (Figure \ref{fig:regular-grids}), $Q = N_{y}$, $P = N_{x}$ 
and the block indices $q$ and $p$ are given by:
\begin{equation}
q \equiv q(i, j) = \Delta l_{ij}
\label{eq:q-x-oriented}
\end{equation}
and
\begin{equation}
p \equiv p(i, j) = \Delta k_{ij} \: ,
\label{eq:p-x-oriented}
\end{equation}
where $\Delta k_{ij}$ and $\Delta l_{ij}$ (equations \ref{eq:Delta_kij} and \ref{eq:Delta_lij}) 
are defined by integer functions $k(i)$, $k(j)$, $l(i)$ and $l(j)$ given by equations 
\ref{eq:k-x-oriented} and \ref{eq:l-x-oriented}.
For $y$-\textit{oriented grids} (Figure \ref{fig:regular-grids}), $Q = N_{x}$, $P = N_{y}$ and 
the block indices $q$ and $p$ are given by:
\begin{equation}
q \equiv q(i, j) = \Delta k_{ij}
\label{eq:q-y-oriented}
\end{equation}
and
\begin{equation}
p \equiv p(i, j) = \Delta l_{ij} \: ,
\label{eq:p-y-oriented}
\end{equation}
where $\Delta k_{ij}$ and $\Delta l_{ij}$ (equations \ref{eq:Delta_kij} and \ref{eq:Delta_lij}) 
are defined by integer functions $k(i)$, $k(j)$, $l(i)$ and $l(j)$ given by equations 
\ref{eq:k-y-oriented} and \ref{eq:l-y-oriented}.
Equations \ref{eq:q-x-oriented}--\ref{eq:p-y-oriented} show that $q$ varies from $-Q+1$
to $Q-1$ and $p$ from $-P+1$ to $P-1$, regardless of the grid orientation. They differ 
from those presented by \citet{takahashi2020convolutional} due to the absence of the module.

% in review ==========>

Let us consider the small regular grid of $N_{x} = 3$ and $N_{y} = 2$ points shown by
Figure \ref{fig:regular-grids}. This grid may represent observation points 
$(x_{i}, y_{i}, z_{0})$ with constant vertical coordinate $z_{0}$ or equivalent sources
$(x_{j}, y_{j}, z_{c})$ with constant vertical coordinate $z_{c} > z_{0}$. In both cases,
the horizontal coordinates are defined by equations \ref{eq:xi} and \ref{eq:yi}.
Given an index $i$, associated with an observation point, and an index $j$, associated with
an equivalent source, we can compute $\Delta k_{ij}$ (equation \ref{eq:Delta_kij}), 
$\Delta l_{ij}$ (equation \ref{eq:Delta_lij}) and $\tfrac{1}{r_{ij}}$ 
(equation \ref{eq:1_rij_regular}). The matrices $\Delta\mathbf{K}$ and $\Delta\mathbf{L}$ 
having elements $ij$ 
defined by $\Delta k_{ij}$ and $\Delta l_{ij}$, respectively, assume different forms, depending on
the grid orientation. For $x$-oriented grids (Figure \ref{fig:regular-grids}), they are given by:
\begin{equation}
\Delta\mathbf{K} = \begin{bmatrix}
0 &  -1 &  -2 &   0 &  -1 &  -2 \\
1 &   0 &  -1 &   1 &   0 &  -1 \\
2 &   1 &   0 &   2 &   1 &   0 \\
0 &  -1 &  -2 &   0 &  -1 &  -2 \\
1 &   0 &  -1 &   1 &   0 &  -1 \\
2 &   1 &   0 &   2 &   1 &   0 \\
\end{bmatrix}
\label{eq:DK-matrix-x-oriented}
\end{equation}
and
\begin{equation}
\Delta\mathbf{L} = \begin{bmatrix}
0 &   0 &   0 &  -1 &  -1 &  -1 \\
0 &   0 &   0 &  -1 &  -1 &  -1 \\
0 &   0 &   0 &  -1 &  -1 &  -1 \\
1 &   1 &   1 &   0 &   0 &   0 \\
1 &   1 &   1 &   0 &   0 &   0 \\
1 &   1 &   1 &   0 &   0 &   0 \\
\end{bmatrix} \: .
\label{eq:DL-matrix-x-oriented}
\end{equation}
For $y$-oriented grids (Figure \ref{fig:regular-grids}), they are given by:
\begin{equation}
\Delta\mathbf{K} = \begin{bmatrix}
0 &   0 &  -1 &  -1 &  -2 &  -2 \\
0 &   0 &  -1 &  -1 &  -2 &  -2 \\
1 &   1 &   0 &   0 &  -1 &  -1 \\
1 &   1 &   0 &   0 &  -1 &  -1 \\
2 &   2 &   1 &   1 &   0 &   0 \\
2 &   2 &   1 &   1 &   0 &   0 \\
\end{bmatrix}
\label{eq:DK-matrix-y-oriented}
\end{equation}
and
\begin{equation}
\Delta\mathbf{L} = \begin{bmatrix}
0 &  -1 &   0 &  -1 &   0 &  -1 \\
1 &   0 &   1 &   0 &   1 &   0 \\
0 &  -1 &   0 &  -1 &   0 &  -1 \\
1 &   0 &   1 &   0 &   1 &   0 \\
0 &  -1 &   0 &  -1 &   0 &  -1 \\
1 &   0 &   1 &   0 &   1 &   0 \\
\end{bmatrix} \: .
\label{eq:DL-matrix-y-oriented}
\end{equation}
These examples (equations \ref{eq:DK-matrix-x-oriented}--\ref{eq:DL-matrix-y-oriented})
show that different combinations of indices $i$ and $j$ result in integer functions 
$\Delta k_{ij}$ and $\Delta l_{ij}$ (equations \ref{eq:Delta_kij} and \ref{eq:Delta_lij}) 
having the same numerical value. In these cases, not only the numerical values of
the corresponding elements $a^{\alpha\beta}_{ij}$ (equation \ref{eq:aij_alpha_beta}),
but also their associated block indices $q$ and $p$ (equations 
\ref{eq:q-x-oriented}--\ref{eq:p-y-oriented}) are the same. 
The contrary is also true: elements $a^{\alpha\beta}_{ij}$ having different 
associated block indices $q$ and $p$ also have different numerical values.
Because of that, using the alternative notation $a^{\alpha\beta}_{qp}$ to define the elements 
$a^{\alpha\beta}_{ij}$ in terms of its associated block indices $q$ and $p$ is a good
approach to investigate the structure of a given matrix component 
$\mathbf{A_{\boldsymbol{\alpha\beta}}}$ (equation \ref{eq:A_expand}).
This approach allows identifying elements $a^{\alpha\beta}_{ij}$ having the same numerical
value only by inspecting their associated block indices.

Note that, for $x$-oriented grids, matrices $\Delta\mathbf{K}$ (equation \ref{eq:DK-matrix-x-oriented})
and $\Delta\mathbf{L}$ (equation \ref{eq:DL-matrix-x-oriented}) define the block indices
$p$ (equation \ref{eq:p-x-oriented}) and $q$ (equation \ref{eq:q-x-oriented}), respectively.
In this case, they are composed of $Q \times Q$ blocks with $P \times P$ elements each, where 
$Q = N_{y}$ and $P = N_{x}$. 
For $y$-oriented grids, matrices $\Delta\mathbf{K}$ (equation \ref{eq:DK-matrix-y-oriented})
and $\Delta\mathbf{L}$ (equation \ref{eq:DL-matrix-y-oriented}) define the block indices
$q$ (equation \ref{eq:q-y-oriented}) and $p$ (equation \ref{eq:p-y-oriented}), respectively.
In this case, they are also composed of $Q \times Q$ blocks with $P \times P$ elements each, 
but now $Q = N_{x}$ and $P = N_{y}$.
The examples shown by equations \ref{eq:DK-matrix-x-oriented}--\ref{eq:DL-matrix-y-oriented}
also illustrate that, regardless of grid orientation, (i) the block index $q$ is constant 
inside each block; (ii) blocks disposed along the same block diagonal are equal to each other; 
(iii) the block index $p$ is constant on each diagonal of a given block; 
(iv) elements of a given block located on the same diagonal are also equal do each other.
The results obtained with the small grid shown in Figure \ref{fig:regular-grids}
can be easily generalized for larger grids.
Based on the well-defined structure of block indices, we can define 
matrices $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ in a general form
\begin{equation}
\mathbf{A}_{\boldsymbol{\alpha\beta}} = \begin{bmatrix}
\mathbf{A}_{\boldsymbol{\alpha\beta}}^{0}   & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{-1} & \cdots          & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{-Q+1} \\
\mathbf{A}_{\boldsymbol{\alpha\beta}}^{1}   & \ddots          & \ddots          & \vdots           \\ 
\vdots           & \ddots          & \ddots          & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{-1}   \\
\mathbf{A}_{\boldsymbol{\alpha\beta}}^{Q-1} & \cdots          & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{1}  & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{0}
\end{bmatrix}_{N \times N} \: ,
\label{eq:BTTB_A_alpha_beta}
\end{equation}
with blocks $\mathbf{A}_{\boldsymbol{\alpha\beta}}^{q}$, $q = -Q+1, \dots, Q-1$, given by
\begin{equation}
\mathbf{A}_{\boldsymbol{\alpha\beta}}^{q} = \begin{bmatrix}
a^{\alpha\beta}_{q0}   & a^{\alpha\beta}_{q(-1)} & \cdots  & a^{\alpha\beta}_{q(-P+1)} \\
a^{\alpha\beta}_{q1}   & \ddots     & \ddots  & \vdots       \\ 
\vdots      & \ddots     & \ddots  & a^{\alpha\beta}_{q(-1)}   \\
a^{\alpha\beta}_{q(P-1)} & \cdots     & a^{\alpha\beta}_{q1}  & a^{\alpha\beta}_{q0}
\end{bmatrix}_{P \times P} \: ,
\label{eq:Aq_block}
\end{equation}
formed by elements $a^{\alpha\beta}_{qp}$, $p = -P+1, \dots, P-1$.
This well-defined structure (equations \ref{eq:BTTB_A_alpha_beta} and \ref{eq:Aq_block}) 
of matrix components $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ 
(equation \ref{eq:A_expand}) is called Block-Toeplitz Toeplitz-Block (BTTB) 
\citep[e.g., ][ p. 67]{chan-jin2007}.

%=====================================================================================================
\subsection{Detailed structure of matrices $\mathbf{A_{xx}}$, $\mathbf{A_{yy}}$ and $\mathbf{A_{zz}}$}
%=====================================================================================================

Equations \ref{eq:BTTB_A_alpha_beta} and \ref{eq:Aq_block} define the general BTTB
structure of all matrix components $\mathbf{A_{\boldsymbol{\alpha\beta}}}$, but 
there are some differences between them.
Let us consider the matrix component $\mathbf{A}_{\boldsymbol{xx}}$, with elements
$a^{xx}_{ij}$ (equation \ref{eq:aij_alpha_beta}) defined by the second derivative
$h^{xx}_{ij}$ (equation \ref{eq:hxx_regular}). It can be easily verified from equations
\ref{eq:Delta_kij_symmetry} and \ref{eq:1_rij_symmetry} that $h^{xx}_{ij} = h^{xx}_{ji}$.
As a consequence, $a^{xx}_{ij} = a^{xx}_{ji}$, which means that 
\begin{equation}
\mathbf{A}_{\boldsymbol{xx}} = \left( \mathbf{A}_{\boldsymbol{xx}} \right)^{\top}
\label{eq:Axx_symmetry}
\end{equation}
for any grid orientation.
Now, let us investigate the elements $a^{xx}_{qp}$ forming the blocks $\mathbf{A}_{\boldsymbol{xx}}^{q}$.
For $x$-oriented grids (Figure \ref{fig:regular-grids}), the block indices $q$ and $p$ are defined 
by equations \ref{eq:q-x-oriented} and 
\ref{eq:p-x-oriented} and $a^{xx}_{qp}$ can be rewritten as follows:
\begin{equation}
a^{xx}_{qp} = c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{x} u_{x} \right) \frac{3 \left( p \, \Delta_{x} \right)^{2}}{r_{qp}^{5}} - 
\frac{1}{r_{qp}^{3}} \: ,
\label{eq:aqp_xx_x_oriented}
\end{equation}
where
\begin{equation}
\frac{1}{r_{qp}} = 
\frac{1}{\sqrt{\left( p \, \Delta_{x} \right)^{2} + \left( q \, \Delta_{y} \right)^{2} + \Delta_{z}^{2}}} \: .
\label{eq:1_rqp_x_oriented}
\end{equation}
For $y$-oriented grids (Figure \ref{fig:regular-grids}), the block indices $q$ and $p$ are 
defined by equations \ref{eq:q-y-oriented} and 
\ref{eq:p-y-oriented} and $a^{xx}_{qp}$ can be rewritten as follows:
\begin{equation}
a^{xx}_{qp} = c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{x} u_{x} \right) \frac{3 \left( q \, \Delta_{x} \right)^{2}}{r_{qp}^{5}} - 
\frac{1}{r_{qp}^{3}} \: ,
\label{eq:aqp_xx_y_oriented}
\end{equation}
where
\begin{equation}
\frac{1}{r_{qp}} = 
\frac{1}{\sqrt{\left( q \, \Delta_{x} \right)^{2} + \left( p \, \Delta_{y} \right)^{2} + \Delta_{z}^{2}}} \: .
\label{eq:1_rqp_y_oriented}
\end{equation}
From equations \ref{eq:aqp_xx_x_oriented}--\ref{eq:1_rqp_y_oriented}, we can easily verify that
\begin{equation}
\mathbf{A}_{\boldsymbol{xx}}^{q} = \mathbf{A}_{\boldsymbol{xx}}^{(-q)}
\label{eq:Axx_q_external_block_symmetry}
\end{equation}
and
\begin{equation}
\mathbf{A}_{\boldsymbol{xx}}^{q} = \left(\mathbf{A}_{\boldsymbol{xx}}^{q} \right)^{\top} \: .
\label{eq:Axx_q_internal_block_symmetry}
\end{equation}
Note that these symmetries are valid for 
any grid orientation.
From this results we conclude the matrix component 
$\mathbf{A}_{\boldsymbol{xx}}$ is \textit{symmetric-Block-Toeplitz symmetric-Toeplitz-Block} 
for any grid orientation.
The same reasoning can be used to show that matrices $\mathbf{A}_{\boldsymbol{yy}}$ and
$\mathbf{A}_{\boldsymbol{zz}}$ also have this symmetric structure.

%==========================================================
\subsection{Detailed structure of matrix $\mathbf{A_{xy}}$}
%==========================================================

Let $\mathbf{A}_{\boldsymbol{xy}}$ be a matrix component with elements
$a^{xy}_{ij}$ (equation \ref{eq:aij_alpha_beta}) defined by the second derivative
$h^{xy}_{ij}$ (equation \ref{eq:hxy_regular}). It can be easily verified from equations
\ref{eq:Delta_kij_symmetry}--\ref{eq:1_rij_symmetry} that $h^{xy}_{ij} = h^{xy}_{ji}$.
As a consequence, $a^{xy}_{ij} = a^{xy}_{ji}$, which means that 
\begin{equation}
\mathbf{A}_{\boldsymbol{xy}} = \left( \mathbf{A}_{\boldsymbol{xy}} \right)^{\top}
\label{eq:Axy_symmetry}
\end{equation}
for any grid orientation.
For $x$-oriented grids (Figure \ref{fig:regular-grids}), the block indices $q$ and $p$ are defined 
by equations \ref{eq:q-x-oriented} and 
\ref{eq:p-x-oriented} and $a^{xy}_{qp}$ can be rewritten as follows:
\begin{equation}
a^{xy}_{qp} = c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{x} u_{y} + F_{y} u_{x} \right) \frac{3 \left( p \, \Delta_{x} \right)\left( q \, \Delta_{y} \right)}{r_{qp}^{5}}
\: ,
\label{eq:aqp_xy_x_oriented}
\end{equation}
with $\tfrac{1}{r_{qp}}$ defined by equation \ref{eq:1_rqp_x_oriented}.
For $y$-oriented grids (Figure \ref{fig:regular-grids}), the block indices $q$ and $p$ are 
defined by equations \ref{eq:q-y-oriented} and 
\ref{eq:p-y-oriented} and $a^{xy}_{qp}$ can be rewritten as follows:
\begin{equation}
a^{xy}_{qp} = c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{x} u_{y} + F_{y} u_{x} \right) \frac{3 \left( q \, \Delta_{x} \right)\left( p \, \Delta_{y} \right)}{r_{qp}^{5}} \: ,
\label{eq:aqp_xy_y_oriented}
\end{equation}
with $\tfrac{1}{r_{qp}}$ defined by equation \ref{eq:1_rqp_y_oriented}.
From equations \ref{eq:1_rqp_x_oriented}, \ref{eq:1_rqp_y_oriented}, \ref{eq:aqp_xy_x_oriented} 
and \ref{eq:aqp_xy_y_oriented}, we can show that
\begin{equation}
\mathbf{A}_{\boldsymbol{xy}}^{q} = -\mathbf{A}_{\boldsymbol{xy}}^{(-q)}
\label{eq:Axy_q_external_block_symmetry}
\end{equation}
and 
\begin{equation}
\mathbf{A}_{\boldsymbol{xy}}^{q} = -\left( \mathbf{A}_{\boldsymbol{xy}}^{q} \right)^{\top} \: .
\label{eq:Axy_q_internal_block_symmetry}
\end{equation}
Note that these symmetries are valid for any grid orientation.
From this results we conclude the matrix component 
$\mathbf{A}_{\boldsymbol{xy}}$ is \textit{skew symmetric-Block-Toeplitz skew symmetric-Toeplitz-Block} 
for any grid orientation.

%==================================================================================
\subsection{Detailed structure of matrices $\mathbf{A_{xz}}$ and $\mathbf{A_{yz}}$}
%==================================================================================

Let $\mathbf{A}_{\boldsymbol{xz}}$ be a matrix component with elements
$a^{xz}_{ij}$ (equation \ref{eq:aij_alpha_beta}) defined by the second derivative
$h^{xz}_{ij}$ (equation \ref{eq:hxz_regular}). It can be easily verified from equations
\ref{eq:Delta_kij_symmetry}--\ref{eq:1_rij_symmetry} that $h^{xz}_{ij} = -h^{xz}_{ji}$.
As a consequence, $a^{xz}_{ij} = -a^{xz}_{ji}$, which means that 
\begin{equation}
\mathbf{A}_{\boldsymbol{xz}} = -\left( \mathbf{A}_{\boldsymbol{xz}} \right)^{\top}
\label{eq:Axz_symmetry}
\end{equation} 
for any grid orientation.
For $x$-oriented grids (Figure \ref{fig:regular-grids}), the block indices $q$ and $p$ are defined 
by equations \ref{eq:q-x-oriented} and 
\ref{eq:p-x-oriented} and $a^{xz}_{qp}$ can be rewritten as follows:
\begin{equation}
a^{xz}_{qp} = c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{x} u_{z} + F_{z} u_{x} \right) \frac{3 \left( p \, \Delta_{x} \right) \Delta_{z}}{r_{qp}^{5}}
\: ,
\label{eq:aqp_xz_x_oriented}
\end{equation}
with $\tfrac{1}{r_{qp}}$ defined by equation \ref{eq:1_rqp_x_oriented}.
In this case, we can see that
\begin{equation}
\mathbf{A}_{\boldsymbol{xz}}^{q} = \mathbf{A}_{\boldsymbol{xz}}^{(-q)}
\label{eq:Axz_q_external_block_symmetry_x_oriented}
\end{equation}
and 
\begin{equation}
\mathbf{A}_{\boldsymbol{xz}}^{q} = -\left( \mathbf{A}_{\boldsymbol{xz}}^{q} \right)^{\top} \: .
\label{eq:Axz_q_internal_block_symmetry_x_oriented}
\end{equation}
This structure is called \textit{symmetric-Block-Toeplitz skew symmetric-Toeplitz-Block} and is 
valid only for $x$-oriented grids.
For $y$-oriented grids (Figure \ref{fig:regular-grids}), the block indices $q$ and $p$ are 
defined by equations \ref{eq:q-y-oriented} and 
\ref{eq:p-y-oriented} and $a^{xz}_{qp}$ can be rewritten as follows:
\begin{equation}
a^{xz}_{qp} = c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{x} u_{z} + F_{z} u_{x} \right) \frac{3 \left( q \, \Delta_{x} \right) \Delta_{z}}{r_{qp}^{5}} \: ,
\label{eq:aqp_xz_y_oriented}
\end{equation}
with $\tfrac{1}{r_{qp}}$ defined by equation \ref{eq:1_rqp_y_oriented}.
Now, we conclude that
\begin{equation}
\mathbf{A}_{\boldsymbol{xz}}^{q} = -\mathbf{A}_{\boldsymbol{xz}}^{(-q)}
\label{eq:Axz_q_external_block_symmetry_y_oriented}
\end{equation}
and 
\begin{equation}
\mathbf{A}_{\boldsymbol{xz}}^{q} = \left( \mathbf{A}_{\boldsymbol{xz}}^{q} \right)^{\top} \: .
\label{eq:Axz_q_internal_block_symmetry_y_oriented}
\end{equation}
This structure is called \textit{skew symmetric-Block-Toeplitz symmetric-Toeplitz-Block} and is 
valid only for $y$-oriented grids.

The same reasoning can be followed to show that
\begin{equation}
\mathbf{A}_{\boldsymbol{yz}} = -\left( \mathbf{A}_{\boldsymbol{yz}} \right)^{\top}
\label{eq:Ayz_symmetry}
\end{equation} 
for any grid orientation. Besides, we can also show that
\begin{equation}
\mathbf{A}_{\boldsymbol{yz}}^{q} = -\mathbf{A}_{\boldsymbol{yz}}^{(-q)}
\label{eq:Ayz_q_external_block_symmetry_x_oriented}
\end{equation}
and 
\begin{equation}
\mathbf{A}_{\boldsymbol{yz}}^{q} = \left( \mathbf{A}_{\boldsymbol{yz}}^{q} \right)^{\top}
\label{eq:Ayz_q_internal_block_symmetry_x_oriented}
\end{equation}
for $x$-oriented grids (\textit{skew symmetric-Block-Toeplitz symmetric-Toeplitz-Block}), while
\begin{equation}
\mathbf{A}_{\boldsymbol{yz}}^{q} = \mathbf{A}_{\boldsymbol{yz}}^{(-q)}
\label{eq:Ayz_q_external_block_symmetry_y_oriented}
\end{equation}
and 
\begin{equation}
\mathbf{A}_{\boldsymbol{yz}}^{q} = -\left( \mathbf{A}_{\boldsymbol{yz}}^{q} \right)^{\top}
\label{eq:Ayz_q_internal_block_symmetry_y_oriented}
\end{equation}
for $y$-oriented grids (\textit{symmetric-Block-Toeplitz skew symmetric-Toeplitz-Block}).

% in review ==========>

%======================================================================================
%\subsection{Standard Conjugate Gradient Least Squares (CGLS) method}
\subsection{Convolutional equivalent layer}
%======================================================================================

The computational cost associated with the classical method to estimate the parameter 
vector $\mathbf{p}$ by solving the linear system \ref{eq:normal-equations} can be very high 
or even prohibitive when dealing with large data sets. In these cases, a well-known alternative
is solving the normal equations (equation \ref{eq:normal-equations}) iteratively by 
using the \textit{standard Conjugate Gradient Least Squares (CGLS) method}:

\begin{algorithm}[H]
	Input: $\mathbf{A}$ and $\mathbf{d}^{o}$.
	
	Output: Estimated parameter vector $\tilde{\mathbf{p}}$.
	
	Set $it = 0$, $\tilde{\mathbf{p}}_{(it)} = \mathbf{0}$, $\mathbf{c}_{(it-1)} = \mathbf{0}$, $\beta_{(it)} = 0$, $\mathbf{s}_{(it)} = \mathbf{d}^{o}$ and $\mathbf{r}_{(it)} = \mathbf{A}^{\top} \mathbf{s}_{(it)}$.
	
	1 - If $it > 0$, $\beta_{(it)} = \dfrac{\| \mathbf{r}_{(it)} \|_{2}^{2}}{\| \mathbf{r}_{(it - 1)} \|_{2}^{2}}$
	
	2 - $\mathbf{c}_{(it)} = \mathbf{r}_{(it)} + \beta_{(it)} \, \mathbf{c}_{(it - 1)}$
	
	3 - $\alpha_{(it)} = \dfrac{{\| \mathbf{r}_{(it)}\|_{2}^{2}}}{\| \mathbf{A} \, \mathbf{c}_{(it)} \|_{2}^{2}}$
	
	4 - $\tilde{\mathbf{p}}_{(it + 1)} = \tilde{\mathbf{p}}_{(it)} + \alpha_{(it)} \, \mathbf{c}_{(it)}$
	
	5 - $\mathbf{s}_{(it + 1)} = \mathbf{s}_{(it)} - \alpha_{(it)} \, \mathbf{A} \, \mathbf{c}_{(it)}$
	
	6 - $\mathbf{r}_{(it + 1)} = \mathbf{A}^{\top} \, \mathbf{s}_{(it + 1)}$
	
	7 - $it = it + 1$
	
	8 - Repeat previous steps until convergence.
	
	\caption{Standard CGLS pseudocode \citep[][ p. 166]{aster2019parameter}.}
\label{al:std-cgls-algorithm}
\end{algorithm}

Setting a convergence criteria based on the minimum tolerance of the residuals is a good 
option to carry out this algorithm efficiently and still obtaining very good results. 
Another possibility is to set an invariance to the Euclidean norm of residuals between 
iterations, which would increase algorithm runtime, but with smaller residuals. 
We chose the first option, as we achieve satisfactory results. 

Note that the standard CGLS solution (Algorithm \ref{al:std-cgls-algorithm}) requires 
neither inverse matrix nor matrix-matrix product. Instead, it only requires: one matrix-vector 
product out of the loop and two matrix-vector products per iteration (in steps 3 and 6). 
These products can be efficiently computed by using the 2D FFT, as a discrete convolution
(see Appendix A). \citet{takahashi2020convolutional} used this approach
to develop an efficient algorithm for gravity data processing. This modified approach in which
the standard CGLS method is modified to compute the matrix-vector products efficiently will be 
referenced throughout this work as the \textit{convolutional equivalent layer method}.


PAREI AQUI






%======================================================================================
\subsection{CGLS matrix-vector substitution}
%======================================================================================

Also differently for the symmetric sensitivity matrix described by \cite{takahashi2020convolutional}, the non-symmetric BTTB matrix cannot be reconstructed only by its first column. The construction of the matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) needs four columns: the first and last columns of the first column of blocks and the first and last columns of the last column of blocks. This has a physical implication in the equivalent layer which is not possible to use only one equivalent source to reprduce all elements of matrix $\mathbf{A}$, such as in the gravity case as demonstrared by \cite{takahashi2020convolutional}. Rather, in the magnetic case it takes four equivalent sources positioned at each corner of the equivalent layer. Figure \ref{fig:4_equivalent_sources} shows the positioning of the equivalent sources in a regular grid $N_x = 4$
and $N_y = 3$ necessary to calculate the four columns capable of recover the matrix $\mathbf{A}$.

In this work, we propose a different approach, by calculating the first column of all six different components of second derivatives matrices from $\mathbf{H}_{ij}$ (equation \ref{eq:Hi}). These matrices are, in fact, symmetrics or skew-symmetrics BTTBs, meaning that the first column has all elements of each matrix.

As pointed earlier in this work, the main improvement inside the CGLS method (Algorithm \ref{al:std-cgls-algorithm}) for estimating the parameter vector $\hat{\mathbf{p}}$ (equation \ref{eq:estimated-p-parameter-space}) is to substitute the matrix-vector multiplication $\mathbf{A}^{\top} \mathbf{s}^{(0)}$ out of the loop and the two matrix-vector multiplications inside the loop at steps 3 an 6, $\mathbf{A} \, \mathbf{c}^{(it)}$ and $\mathbf{A}^{\top} \, \mathbf{s}^{(it + 1)}$, that is necessary at each iteration and takes most of its runtime.

Our method consists in calculating the six first columns of the second derivatives of $\mathbf{H}$ (equation \ref{eq:Hi}) and embedding them into the first six columns of the block-circulant circulant-block (BCCB) matrices related to the $\mathbf{H}$ components. Thus, it is possible to calculate the first column of the BCCB matrix embbeded from matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) by multiplying each component with its respective constants and summing as shown in equation \ref{eq:aij_mag_expand}. In \cite{takahashi2020convolutional}, Appendix A, the authors demonstrated in details how to transform a symmetric BTTB matrix into a BCCB matrix $\mathbf{C}$. The process here is the same and that work can be referenced to achieve the same results.



%======================================================================================
\subsection{Computational performance}
%======================================================================================

To compare the efficiency of our algorithm we will use a numerical approach and calculate the floating-point operations (\emph{flops}), i.e., count the number of mathematical operations necessary to complete the estimative of parameter vector $\mathbf{\hat{p}}$ of the normal equations (equation \ref{eq:estimated-p-parameter-space}) and both the CGLS methods (algorithm \ref{al:std-cgls-algorithm}) for calculating the matrix-vector product by its standart way and our approach.

The \emph{flops} needed to solve the linear system in equation \ref{eq:estimated-p-parameter-space} using the Cholesky factorization is:
\begin{equation}
f_{classical} =  \dfrac{7}{3} N^{3} + 6 N^{2}\: ,
\label{eq:flops-normal-cholesky}
\end{equation}
where $N$ is the total number of observation points and also the size of estimated parameter vector $\mathbf{\hat{p}}$.

For the more efficient CGLS algorithm the estimative can be done in:
\begin{equation}
f_{cgls} =  2 N^{2} + it \, (4 N^{2} + 12 N) \: .
\label{eq:flops-cgls}
\end{equation}
However, our approach reduces further to:
\begin{equation}
f_{ours} =  \kappa  \, 16 N \log_2(4 N) + 24 N + it \, (\kappa  \, 16 N \log_2 (4 N) + 60 N) \: ,
\label{eq:flops-cgls-bccb}
\end{equation}
where $\kappa$ depends on the FFT algorithm. By default, in this work we will use $\kappa = 5$ for the \emph{radix-2} algorithm \citep{vanloan1992}.

Figure \ref{fig:flops} shows a comparative between the methods varying the number of observation points up to $1,000,000$, where it is possible to observe a reduction of $10^7$ orders of magnitude to estimate parameter vector $\mathbf{\hat{p}}$ in relation to the non-iterative classical method and $10^3$ orders of magnitude in relation to the standart CGLS algorithm using $50$ iterations. A more detailed, step by step, flops count of the classical and CGLS algorithm can be found in Appendix A.

In Figure \ref{fig:solve_time} we show the time necessary to construct matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) and solve the linear system up to $10,000$ points of observation. With this dataset the classical method takes more than sixty-three seconds, the CGLS more than twelve seconds, while our method takes only half a second. The cpu used for this test was a intel core i7-7700HQ@2.8GHz.

In Figure \ref{fig:sources_time} a comparison between the time to complete the task to calculate the first column of the BCCB matrix embbeded from the from $\mathbf{A}$ (equation \ref{eq:aij_mag}) by using only one equivalent source, i.e., calculating all six first column of the second derivatives matrices from $\mathbf{H}$ (equation \ref{eq:Hi}) and using four equivalent sources to calculate the four necessary columns from the non-symmetric matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}). Although, very similar in time, with one source a small advantage can be observed as the number of data $N$ increases and goes beyond $N = 200,000$. This test was done from $N = 10,000$ to $N = 700,000$ with increases of $5,625$ observation points.

In Table \ref{tab:RAM-usage} there is comparison between how much RAM memory is adressed to store the sensitivity matrix for each of the methods. The classical approach and the CGLS have to store the whole matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}), this means that a dataset  with for example $N = 10,000$ observation points, the sensitivity matrix has $N^2 = 100,000,000$ elements and takes approximately $763$ Megabytes of memory (8 bytes per element). For our method, it is necessary to store the first six columns of each of the components from matrix $\mathbf{H}$ (equation \ref{eq:Hi}) embedded into the BCCB matrices. With the same dataset $N = 10,000$ it needs $1.831$ Megabytes. After completing the steps to store the eigenvalues of matrix $\mathbf{C}$ (equation \ref{eq:w_Cv}) it takes only $0.6104$ Megabytes. Here, we are considering 16 bytes per element as the eigenvalues are complex numbers resulting from the 2D FFT. For a bigger dataset as $N = 1,000,000$ the amount of RAM necessary goes to $7,629,395$, $183.096$ and $61.035$ Megabytes, respectively, showing the necessity to find improved and efficient methods for the equivalent layer technique as the one presented in this work. We remember that throughout our work we are always considering $N = M$.

% <========== in review







