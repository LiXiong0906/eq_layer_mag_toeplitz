\section{Methodology}

%======================================================================================
\subsection{Classical equivalent layer for magnetic data}
%======================================================================================

Let $\mathbf{d}^{o}$ be the $N \times 1$ observed data vector, whose $i$th element 
is the total-field anomaly $d^{o}_{i}$ produced by arbitrarily magnetized sources
at the position $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $, 
of a right-handed Cartesian coordinate system with $x$-, $y$- and $z$-axis 
pointing to north, east and down, respectively.
We consider that the total-field anomaly data $d^{o}_{i}$ represent the discrete
values of a harmonic function. Besides, we consider that the main geomagnetic field 
direction at the study area can be defined by the unit vector
\begin{equation}
\hat{\mathbf{F}} = \begin{bmatrix}
F_x \\
F_y \\
F_z
\end{bmatrix} =
\begin{bmatrix}
\cos(I_{0}) \, \cos(D_{0}) \\
\cos(I_{0}) \, \sin(D_{0}) \\
\sin(I_{0})
\end{bmatrix} \: ,
\label{eq:unit_vector_F}
\end{equation}
with constant inclination $I_{0}$ and declination $D_{0}$.
In this case, $d^{o}_{i}$ can be approximated by the predicted total-field anomaly
\begin{equation}
\Delta T_{i} = \sum_{j=1}^{M} \, p_{j} a_{ij} \: ,
\label{eq:integral-sum_mag}
\end{equation}
which describes the magnetic induction exerted, at the observation point $(x_{i}, y_{i}, z_{i})$,
by a discrete layer of $M$ dipoles (equivalent sources) defined on the horizontal plane $z = z_{c}$, 
where $p_{j}$ is the magnetic moment intensity (in A~m~$^{2}$)~of the $j$th dipole, 
that has unit volume and is located at the point $(x_{j}, y_{j}, z_{c})$. In equation
\ref{eq:integral-sum_mag}, $a_{ij}$ is the harmonic function
\begin{equation}
a_{ij}
= c_{m} \, \frac{\mu_{0}}{4\pi} \, \hat{\mathbf{F}}^{\top} \mathbf{H}_{ij} \: \hat{\mathbf{u}} \: ,
\label{eq:aij_mag}
\end{equation}
the unit vector
\begin{equation}
\hat{\mathbf{u}} = \begin{bmatrix}
u_x \\
u_y \\
u_z
\end{bmatrix} =
\begin{bmatrix}
\cos(I) \, \cos(D) \\
\cos(I) \, \sin(D) \\
\sin(I)
\end{bmatrix} \: ,
\label{eq:u_hat}
\end{equation}
defines the magnetization direction of all dipoles, with constant inclination $I$ and declination $D$,
$\mu_{0} = 4\pi \, 10^{-7}$ H/m is the magnetic constant, $c_{m} = 10^{9}$ is a factor that transforms
the magnetic induction from Tesla (T) to nanotesla (nT) and $\mathbf{H}_{ij}$ is a $3 \times 3$ matrix 
\begin{equation}
\mathbf{H}_{ij} = \begin{bmatrix}
h^{xx}_{ij} & h^{xy}_{ij} & h^{xz}_{ij} \\
h^{xy}_{ij} & h^{yy}_{ij} & h^{yz}_{ij} \\
h^{xz}_{ij} & h^{yz}_{ij} & h^{zz}_{ij}
\end{bmatrix} \: ,
\label{eq:Hij}
\end{equation}
where 
\begin{equation}
h^{\alpha\beta}_{ij} = 
\begin{cases}
\frac{3 \left( \alpha_{i} - \alpha_{j} \right)^{2}}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: , \quad \alpha = \beta \\
\frac{3 \left( \alpha_{i} - \alpha_{j} \right) \left( \beta_{i} - \beta_{j} \right)}{r_{ij}^{5}} \: , \quad \alpha \ne \beta
\end{cases} \: , \quad \alpha, \beta = x, y, z \: ,
\label{eq:hij_alpha_beta}
\end{equation}
are the second derivatives of the inverse distance function
\begin{equation}
\frac{1}{r_{ij}} = 
\frac{1}{\sqrt{\left(x_{i} - x_{j} \right)^{2} + 
\left(y_{i} - y_{j} \right)^{2} + \left(z_{i} - z_{c} \right)^{2}}}
\label{eq:1_rij}
\end{equation}
with respect to the coordinates of the observation point $(x_{i}, y_{i}, z_{i})$.

Equation \ref{eq:integral-sum_mag} can be rewritten in matrix notation as follows:
\begin{equation}
\mathbf{d}(\mathbf{p}) = \mathbf{A} \mathbf{p} \: ,
\label{eq:predicted-data-vector_mag}
\end{equation}
where $\mathbf{d}(\mathbf{p})$ is the $N \times 1$ predicted data vector with $i$th element defined
be the predicted total-field anomaly $\Delta T_{i}$ (equation \ref{eq:integral-sum_mag}),
$\mathbf{p}$ is the $M \times 1$ parameter vector whose $j$th element is the magnetic moment intensity
$p_{j}$ of the $j$th dipole and $\mathbf{A}$ is the $N \times M$ sensitivity matrix with element 
$ij$ defined by the harmonic function $a_{ij}$ (equation \ref{eq:aij_mag}).
In the classical equivalent-layer technique, the common approach for 
estimating the parameter vector $\mathbf{p}$ from the observed 
total-field anomaly data $\mathbf{d}^{o}$ is solving the least-squares normal equations
\begin{equation}
\mathbf{A}^{\top}\mathbf{A} \: \mathbf{p} = 
\mathbf{A}^{\top} \mathbf{d}^{o} \: .
\label{eq:normal-equations}
\end{equation}
This equation is usually solved by first computing the Cholesky factor of matrix
$\mathbf{A}^{\top}\mathbf{A}$ and then using it to solve the linear system
\citep[][ p. 262]{golub-vanloan2013}. 
The estimated parameter vector obtained by following this approach will be 
referenced throughout this work as the \textit{classical solution}.

The computational cost required for estimating the classical solution can be very high
when dealing with large datasets. In the following subsections, we will show how to 
explore the structure of the sensitivity matrix $\mathbf{A}$ and 
efficiently solve the least-squares normal equations (equation \ref{eq:normal-equations}).


%=====================================================================================================
\subsection{Matrix $\mathbf{A}$ in terms of matrix components $\mathbf{A_{\boldsymbol{\alpha\beta}}}$}
%=====================================================================================================

To access the structure of the sensitivity matrix $\mathbf{A}$ 
(equation \ref{eq:predicted-data-vector_mag}), let us first rewrite its elements 
$a_{ij}$ (equation \ref{eq:aij_mag}) in the following way:
\begin{equation}
\begin{split}
a_{ij} = a^{xx}_{ij} + a^{xy}_{ij} + a^{xz}_{ij} + a^{yy}_{ij} + a^{yz}_{ij} + a^{zz}_{ij} \: ,
\end{split}
\label{eq:aij_mag_expand}
\end{equation}
where
\begin{equation}
a^{\alpha\beta}_{ij} = 
\begin{cases}
c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{\alpha} u_{\beta} \right) h^{\alpha\beta}_{ij} \: &, \quad \alpha = \beta \\
c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{\alpha} u_{\beta} + F_{\beta} u_{\alpha} \right) h^{\alpha\beta}_{ij} \: &, \quad \alpha \ne \beta \\
\end{cases}
\: , \quad \alpha, \beta = x, y, z \: ,
\label{eq:aij_alpha_beta}
\end{equation}
are defined by the elements of $\hat{\mathbf{F}}$ 
(equation \ref{eq:unit_vector_F}), $\hat{\mathbf{u}}$ (equation \ref{eq:u_hat}) and 
$\mathbf{H}_{ij}$ (equations \ref{eq:Hij} and \ref{eq:hij_alpha_beta}).
Then, we can rewrite the sensitivity matrix $\mathbf{A}$ 
(equation \ref{eq:predicted-data-vector_mag}) according to:
\begin{equation}
\mathbf{A} = \mathbf{A_{xx}} + \mathbf{A_{xy}} + \mathbf{A_{xz}} + 
\mathbf{A_{yy}} + \mathbf{A_{yz}} + \mathbf{A_{zz}} \: ,
\label{eq:A_expand}
\end{equation}
where $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ are $N \times M$ matrices with elements 
$ij$ defined by $a^{\alpha\beta}_{ij}$ (equation \ref{eq:aij_alpha_beta}).

Now we can define the structure of $\mathbf{A}$ in terms of its components 
$\mathbf{A_{\boldsymbol{\alpha\beta}}}$ (equation \ref{eq:A_expand}). To do this, 
we consider the particular case in which the observed total-field anomaly is located 
on an $N_{x} \times N_{y}$ 
regular grid of points spaced by $\Delta_{x}$ and $\Delta_{y}$ along the $x$- and $y$-directions,
respectively, on a constant vertical coordinate $z_{0}$. We also consider that the equivalent layer
is formed by one dipole right below each observation point, at the constant coordinate $z_{c}$.
In this case, the number of equivalent sources $M$ is equal to the number of data $N$ and, 
consequently, matrices $\mathbf{A}$ and $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ become 
square ($N \times N$). 
Besides, the horizontal coordinates $x_{i}$ and $y_{i}$ of the observation points 
can be defined by
\begin{equation}
x_{i} = x_{1} + \left[ k(i) - 1 \right] \, \Delta_{x}
\label{eq:xi}
\end{equation}
and
\begin{equation}
y_{i} = y_{1} + \left[ l(i) - 1 \right] \, \Delta_{y} \: ,
\label{eq:yi}
\end{equation}
where $x_{1}$ and $y_{1}$ are the lower limits for $x_{i}$ and $y_{i}$, respectively,
and $k(i)$ and $l(i)$ are integer functions defined according to the orientation
of the data grid (Figure \ref{fig:regular-grids}). 
For $x$-\textit{oriented grids}, the integer functions are given by
\begin{equation}
k(i)  = i - \Bigg\lceil \frac{i}{N_{x}} \Bigg\rceil N_{x} + N_{x}
\label{eq:k-x-oriented}
\end{equation}
and
\begin{equation}
l(i) = \Bigg\lceil \frac{i}{N_{x}} \Bigg\rceil \: .
\label{eq:l-x-oriented}
\end{equation}
For $y$-\textit{oriented grids}, the integer functions are given by
\begin{equation}
k(i) = \Bigg\lceil \frac{i}{N_{y}} \Bigg\rceil
\label{eq:k-y-oriented}
\end{equation}
and
\begin{equation}
l(i) = i - \Bigg\lceil \frac{i}{N_{y}} \Bigg\rceil N_{y} + N_{y} \: .
\label{eq:l-y-oriented}
\end{equation}
In equations \ref{eq:k-x-oriented}--\ref{eq:l-y-oriented}, $\lceil \cdot \rceil$ denotes the ceiling 
function \citep[e.g.,][ p. 67-68]{graham-etal1994}.
Equations \ref{eq:xi}--\ref{eq:l-y-oriented} can also be used to define the coordinates 
$x_{j}$ and $y_{j}$ of the equivalent sources, but with index $j$ instead of $i$.

By using equations \ref{eq:xi}--\ref{eq:l-y-oriented} to define the coordinates $x_{i}$ and 
$y_{i}$ of the observation points and $x_{j}$ and $y_{j}$ of the equivalent sources, we can
rewrite the elements $h^{\alpha\beta}_{ij}$ (equation \ref{eq:hij_alpha_beta}) of matrix 
$\mathbf{H}_{ij}$ (equation \ref{eq:Hij}) as follows:
\begin{equation}
h^{xx}_{ij} = 
\frac{3 \left( \Delta k_{ij} \, \Delta_{x} \right)^{2}}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: ,
\label{eq:hxx_regular}
\end{equation}
\begin{equation}
h^{yy}_{ij} = 
\frac{3 \left( \Delta l_{ij} \, \Delta_{y} \right)^{2}}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: ,
\label{eq:hyy_regular}
\end{equation}
\begin{equation}
h^{zz}_{ij} = 
\frac{3 \Delta_{z}^{2}}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: ,
\label{eq:hzz_regular}
\end{equation}
\begin{equation}
h^{xy}_{ij} = 
\frac{3 \left( \Delta k_{ij} \, \Delta_{x} \right)\left( \Delta l_{ij} \, \Delta_{y} \right)}{r_{ij}^{5}} \: ,
\label{eq:hxy_regular}
\end{equation}
\begin{equation}
h^{xz}_{ij} = 
\frac{3 \left( \Delta k_{ij} \, \Delta_{x} \right) \Delta_{z}}{r_{ij}^{5}}
\label{eq:hxz_regular}
\end{equation}
and
\begin{equation}
h^{yz}_{ij} = 
\frac{3 \left( \Delta l_{ij} \, \Delta_{y} \right) \Delta_{z}}{r_{ij}^{5}} \: ,
\label{eq:hyz_regular}
\end{equation}
where $\Delta_{z} = z_{c} - z_{0}$, 
\begin{equation}
\Delta k_{ij} = \frac{x_{i} - x_{j}}{\Delta_{x}} = k(i) - k(j) \: ,
\label{eq:Delta_kij}
\end{equation}
\begin{equation}
\Delta l_{ij} = \frac{y_{i} - y_{j}}{\Delta_{y}} = l(i) - l(j)
\label{eq:Delta_lij}
\end{equation}
and 
\begin{equation}
\frac{1}{r_{ij}} = 
\frac{1}{\sqrt{\left( \Delta k_{ij} \, \Delta_{x} \right)^{2} + \left( \Delta l_{ij} \, \Delta_{y} \right)^{2} + \Delta_{z}^{2}}} \: .
\label{eq:1_rij_regular}
\end{equation}
Note that the integer functions $k(i)$, $k(j)$, $l(i)$ and $l(j)$ (equations 
\ref{eq:k-x-oriented}--\ref{eq:l-y-oriented}) defining $\Delta k_{ij}$ (equation
\ref{eq:Delta_kij}), $\Delta l_{ij}$ (equation \ref{eq:Delta_lij}) and 
$\tfrac{1}{r_{ij}}$ (equation \ref{eq:1_rij_regular}) assume different 
forms depending on the grid orientation.
Despite of that, it can be shown that
\begin{equation}
\Delta k_{ij} = - \Delta k_{ji} \: ,
\label{eq:Delta_kij_symmetry}
\end{equation}
\begin{equation}
\Delta l_{ij} = - \Delta l_{ji}
\label{eq:Delta_lij_symmetry}
\end{equation}
and 
\begin{equation}
\frac{1}{r_{ij}} = \frac{1}{r_{ji}}
\label{eq:1_rij_symmetry}
\end{equation}
for any grid orientation.

%=================================================================================
\subsection{General structure of matrices $\mathbf{A_{\boldsymbol{\alpha\beta}}}$}
%=================================================================================

By using equations \ref{eq:hxx_regular}--\ref{eq:1_rij_regular} to compute 
$a^{\alpha\beta}_{ij}$ (equation \ref{eq:aij_alpha_beta}), we can show that 
matrices $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ (equation \ref{eq:A_expand}) assume
well-defined structures that can be conveniently
represented by using \textit{block indices} $q$ and $p$ \citep{takahashi2020convolutional}.
These indices are defined by the integer functions $\Delta k_{ij}$ and $\Delta l_{ij}$ 
(equations \ref{eq:Delta_kij} and \ref{eq:Delta_lij}), in terms of the indices $i$ 
of the observation points $(x_{i}, y_{i}, z_{0})$ and $j$ of the equivalent sources
$(x_{j}, y_{j}, z_{c})$.
For $x$-\textit{oriented grids}, $Q = N_{y}$, $P = N_{x}$ and the block indices
$q$ and $p$ are given by:
\begin{equation}
q \equiv q(i, j) = \Delta l_{ij}
\label{eq:q-x-oriented}
\end{equation}
and
\begin{equation}
p \equiv p(i, j) = \Delta k_{ij} \: ,
\label{eq:p-x-oriented}
\end{equation}
where $\Delta k_{ij}$ and $\Delta l_{ij}$ (equations \ref{eq:Delta_kij} and \ref{eq:Delta_lij}) 
are defined by integer functions $k(i)$, $k(j)$, $l(i)$ and $l(j)$ given by equations 
\ref{eq:k-x-oriented} and \ref{eq:l-x-oriented}.
For $y$-\textit{oriented grids}, $Q = N_{x}$, $P = N_{y}$ and the block indices
$q$ and $p$ are given by:
\begin{equation}
q \equiv q(i, j) = \Delta k_{ij}
\label{eq:q-y-oriented}
\end{equation}
and
\begin{equation}
p \equiv p(i, j) = \Delta l_{ij} \: ,
\label{eq:p-y-oriented}
\end{equation}
where $\Delta k_{ij}$ and $\Delta l_{ij}$ (equations \ref{eq:Delta_kij} and \ref{eq:Delta_lij}) 
are defined by integer functions $k(i)$, $k(j)$, $l(i)$ and $l(j)$ given by equations 
\ref{eq:k-y-oriented} and \ref{eq:l-y-oriented}.
Equations \ref{eq:q-x-oriented}--\ref{eq:p-y-oriented} differ from those presented by
\citet{takahashi2020convolutional} in the absence of the module.

% in review ==========>

Let us consider the small regular grid of $N_{x} = 3$ and $N_{y} = 2$ points shown by
Figure \ref{fig:regular-grids}. This grid may represent observation points 
$(x_{i}, y_{i}, z_{0})$ with constant vertical coordinate $z_{0}$ or equivalent sources
$(x_{j}, y_{j}, z_{c})$ with constant vertical coordinate $z_{c} > z_{0}$. In both cases,
the horizontal coordinates are defined by equations \ref{eq:xi} and \ref{eq:yi}.
Given an index $i$, associated with an observation point, and an index $j$, associated with
an equivalent source, we can compute $\Delta k_{ij}$ (equation \ref{eq:Delta_kij}), 
$\Delta l_{ij}$ (equation \ref{eq:Delta_lij}) and $\tfrac{1}{r_{ij}}$ 
(equation \ref{eq:1_rij_regular}). The matrices $\Delta\mathbf{K}$ and $\Delta\mathbf{L}$ 
having elements $ij$ 
defined by $\Delta k_{ij}$ and $\Delta l_{ij}$, respectively, assume different forms, depending on
the grid orientation. For $x$-oriented grids, they are given by:
\begin{equation}
\Delta\mathbf{K} = \begin{bmatrix}
0 &  -1 &  -2 &   0 &  -1 &  -2 \\
1 &   0 &  -1 &   1 &   0 &  -1 \\
2 &   1 &   0 &   2 &   1 &   0 \\
0 &  -1 &  -2 &   0 &  -1 &  -2 \\
1 &   0 &  -1 &   1 &   0 &  -1 \\
2 &   1 &   0 &   2 &   1 &   0 \\
\end{bmatrix}
\label{eq:DK-matrix-x-oriented}
\end{equation}
and
\begin{equation}
\Delta\mathbf{L} = \begin{bmatrix}
0 &   0 &   0 &  -1 &  -1 &  -1 \\
0 &   0 &   0 &  -1 &  -1 &  -1 \\
0 &   0 &   0 &  -1 &  -1 &  -1 \\
1 &   1 &   1 &   0 &   0 &   0 \\
1 &   1 &   1 &   0 &   0 &   0 \\
1 &   1 &   1 &   0 &   0 &   0 \\
\end{bmatrix} \: .
\label{eq:DL-matrix-x-oriented}
\end{equation}
For $y$-oriented grids, they are given by:
\begin{equation}
\Delta\mathbf{K} = \begin{bmatrix}
0 &   0 &  -1 &  -1 &  -2 &  -2 \\
0 &   0 &  -1 &  -1 &  -2 &  -2 \\
1 &   1 &   0 &   0 &  -1 &  -1 \\
1 &   1 &   0 &   0 &  -1 &  -1 \\
2 &   2 &   1 &   1 &   0 &   0 \\
2 &   2 &   1 &   1 &   0 &   0 \\
\end{bmatrix}
\label{eq:DK-matrix-y-oriented}
\end{equation}
and
\begin{equation}
\Delta\mathbf{L} = \begin{bmatrix}
0 &  -1 &   0 &  -1 &   0 &  -1 \\
1 &   0 &   1 &   0 &   1 &   0 \\
0 &  -1 &   0 &  -1 &   0 &  -1 \\
1 &   0 &   1 &   0 &   1 &   0 \\
0 &  -1 &   0 &  -1 &   0 &  -1 \\
1 &   0 &   1 &   0 &   1 &   0 \\
\end{bmatrix} \: .
\label{eq:DL-matrix-y-oriented}
\end{equation}

These examples (equations \ref{eq:DK-matrix-x-oriented}--\ref{eq:DL-matrix-y-oriented})
show that different combinations of indices $i$ and $j$ result in integer functions 
$\Delta k_{ij}$ and $\Delta l_{ij}$ (equations \ref{eq:Delta_kij} and \ref{eq:Delta_lij}) 
having the same numerical value. In these cases, not only the numerical values of
the corresponding elements $a^{\alpha\beta}_{ij}$ (equation \ref{eq:aij_alpha_beta}),
but also their associated block indices $q$ and $p$ (equations 
\ref{eq:q-x-oriented}--\ref{eq:p-y-oriented}) are the same. 
The contrary is also true: elements $a^{\alpha\beta}_{ij}$ having different 
associated block indices $q$ and $p$ also have different numerical values.
Because of that, using the alternative notation $a^{\alpha\beta}_{qp}$ to define the elements 
$a^{\alpha\beta}_{ij}$ in terms of its associated block indices $q$ and $p$ is a good
approach to investigate the structure of a given matrix component 
$\mathbf{A_{\boldsymbol{\alpha\beta}}}$ (equation \ref{eq:A_expand}).
This approach allows identifying elements $a^{\alpha\beta}_{ij}$ having the same numerical
value only by inspecting their associated block indices.



Note that, for $x$-oriented grids, matrices $\Delta\mathbf{K}$ (equation \ref{eq:DK-matrix-x-oriented})
and $\Delta\mathbf{L}$ (equation \ref{eq:DL-matrix-x-oriented}) define the block indices
$p$ (equation \ref{eq:p-x-oriented}) and $q$ (equation \ref{eq:q-x-oriented}), respectively.
In this case, they are composed of $Q \times Q$ blocks with $P \times P$ elements each, where 
$Q = N_{y}$ and $P = N_{x}$. 
For $y$-oriented grids, matrices $\Delta\mathbf{K}$ (equation \ref{eq:DK-matrix-y-oriented})
and $\Delta\mathbf{L}$ (equation \ref{eq:DL-matrix-y-oriented}) define the block indices
$q$ (equation \ref{eq:q-y-oriented}) and $p$ (equation \ref{eq:p-y-oriented}), respectively.
In this case, they are also composed of $Q \times Q$ blocks with $P \times P$ elements each, 
but now $Q = N_{x}$ and $P = N_{y}$.
The examples shown by equations \ref{eq:DK-matrix-x-oriented}--\ref{eq:DL-matrix-y-oriented}
also illustrate that, regardless of grid orientation, (i) the block index $q$ is constant 
inside each block; (ii) blocks disposed along the same block diagonal are equal to each other; 
(iii) the block index $p$ is constant on each diagonal of a given block; 
(iv) elements of a given block located on the same diagonal are also equal do each other.
Based on this well-defined structure of block indices, we can define 
matrices $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ in a general form
\begin{equation}
\mathbf{A}_{\boldsymbol{\alpha\beta}} = \begin{bmatrix}
\mathbf{A}_{\boldsymbol{\alpha\beta}}^{0}   & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{-1} & \cdots          & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{-Q+1} \\
\mathbf{A}_{\boldsymbol{\alpha\beta}}^{1}   & \ddots          & \ddots          & \vdots           \\ 
\vdots           & \ddots          & \ddots          & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{-1}   \\
\mathbf{A}_{\boldsymbol{\alpha\beta}}^{Q-1} & \cdots          & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{1}  & \mathbf{A}_{\boldsymbol{\alpha\beta}}^{0}
\end{bmatrix}_{N \times N} \: ,
\label{eq:BTTB_A_alpha_beta}
\end{equation}
with blocks $\mathbf{A}_{\boldsymbol{\alpha\beta}}^{q}$ given by
\begin{equation}
\mathbf{A}_{\boldsymbol{\alpha\beta}}^{q} = \begin{bmatrix}
a^{\alpha\beta}_{q0}   & a^{\alpha\beta}_{q(-1)} & \cdots  & a^{\alpha\beta}_{q(-P+1)} \\
a^{\alpha\beta}_{q1}   & \ddots     & \ddots  & \vdots       \\ 
\vdots      & \ddots     & \ddots  & a^{\alpha\beta}_{q(-1)}   \\
a^{\alpha\beta}_{q(P-1)} & \cdots     & a^{\alpha\beta}_{q1}  & a^{\alpha\beta}_{q0}
\end{bmatrix}_{P \times P} \: .
\label{eq:Aq_block}
\end{equation}
This well-defined structure (equations \ref{eq:BTTB_A_alpha_beta} and \ref{eq:Aq_block}) 
of matrix components $\mathbf{A_{\boldsymbol{\alpha\beta}}}$ 
(equation \ref{eq:A_expand}) is called Block-Toeplitz Toeplitz-Block (BTTB) 
\citep[e.g., ][ p. 67]{chan-jin2007}.

% in review ==========>

%=====================================================================================================
\subsection{Detailed structure of matrices $\mathbf{A_{xx}}$, $\mathbf{A_{yy}}$ and $\mathbf{A_{zz}}$}
%=====================================================================================================

Equations \ref{eq:BTTB_A_alpha_beta} and \ref{eq:Aq_block} define the general BTTB
structure of all matrix components $\mathbf{A_{\boldsymbol{\alpha\beta}}}$, but 
there are some differences between them.
Let us consider the matrix component $\mathbf{A}_{\boldsymbol{xx}}$, with elements
$a^{xx}_{ij}$ (equation \ref{eq:aij_alpha_beta}) defined by the second derivative
$h^{xx}_{ij}$ (equation \ref{eq:hxx_regular}). It can be easily verified from equations
\ref{eq:Delta_kij_symmetry} and \ref{eq:1_rij_symmetry} that $h^{xx}_{ij} = h^{xx}_{ji}$.
As a consequence, $a^{xx}_{ij} = a^{xx}_{ji}$ and we conclude that $\mathbf{A}_{\boldsymbol{xx}}$ 
is symmetric.
Now, let us investigate the elements $a^{xx}_{qp}$ forming the blocks $\mathbf{A}_{\boldsymbol{xx}}^{q}$.
For $x$-oriented grids, the block indices $q$ and $p$ are defined by equations \ref{eq:q-x-oriented} and 
\ref{eq:p-x-oriented} and $a^{xx}_{qp}$ can be rewritten as follows:
\begin{equation}
a^{xx}_{qp} = c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{x} u_{x} \right) \frac{3 \left( p \, \Delta_{x} \right)^{2}}{r_{qp}^{5}} - 
\frac{1}{r_{qp}^{3}} \: ,
\label{eq:aqp_xx_x_oriented}
\end{equation}
where
\begin{equation}
\frac{1}{r_{qp}} = 
\frac{1}{\sqrt{\left( p \, \Delta_{x} \right)^{2} + \left( q \, \Delta_{y} \right)^{2} + \Delta_{z}^{2}}} \: .
\label{eq:1_rqp_x_oriented}
\end{equation}
For $y$-oriented grids, the block indices $q$ and $p$ are defined by equations \ref{eq:q-y-oriented} and 
\ref{eq:p-y-oriented} and $a^{xx}_{qp}$ can be rewritten as follows:
\begin{equation}
a^{xx}_{qp} = c_{m} \, \frac{\mu_{0}}{4\pi} 
\left( F_{x} u_{x} \right) \frac{3 \left( q \, \Delta_{x} \right)^{2}}{r_{qp}^{5}} - 
\frac{1}{r_{qp}^{3}} \: ,
\label{eq:aqp_xx_y_oriented}
\end{equation}
where
\begin{equation}
\frac{1}{r_{qp}} = 
\frac{1}{\sqrt{\left( q \, \Delta_{x} \right)^{2} + \left( p \, \Delta_{y} \right)^{2} + \Delta_{z}^{2}}} \: .
\label{eq:1_rqp_y_oriented}
\end{equation}
From equations \ref{eq:aqp_xx_x_oriented}--\ref{eq:1_rqp_y_oriented}, we can easily verify that
$a^{xx}_{qp} = a^{xx}_{(-q)p}$ and, consequently, that 
$\mathbf{A}_{\boldsymbol{xx}}^{q} = \mathbf{A}_{\boldsymbol{xx}}^{(-q)}$. We can also verify 
that $a^{xx}_{qp} = a^{xx}_{q(-p)}$ and, consequently, that the blocks 
$\mathbf{A}_{\boldsymbol{xx}}^{q}$ are symmetric. Note that these symmetries are valid for 
any grid orientation.
From this results we conclude the matrix component 
$\mathbf{A}_{\boldsymbol{xx}}$ is symmetric-Block-Toeplitz symmetric-Toeplitz-Block for any 
grid orientation.
The same reasoning can be used to show that matrices $\mathbf{A}_{\boldsymbol{yy}}$ and
$\mathbf{A}_{\boldsymbol{zz}}$ also have this symmetric structure.

PAREI AQUI

%======================================================================================
%\subsection{Structure of the components matrices $\mathbf{H_{xy}}$}
%======================================================================================

By substituting equations \ref{eq:xi} and \ref{eq:yi} in equation \ref{eq:Hi}, we can also describe the elements of $\mathbf{H_{xy}}$, as:
\begin{equation}
h^{xy}_{ij} = \frac{3 (\Delta k_{ij} \, \Delta x )(\Delta l_{ij} \, \Delta y )}{\left[ 
	\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
	\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
	\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: ,
\label{eq:hxy_mag}
\end{equation}

The component $\mathbf{H_{xy}}$ (equation \ref{eq:hxy_mag}) of matrix $\mathbf{H}$ (equation \ref{eq:Hi}) are skew symmetric-Block-Toeplitz skew symmetric-Toeplitz-Block matrices. This means that $\mathbf{H_{xy}}$ is Toeplitz and skew symmetric by its blocks and each of the blocks are skew symmetric Toeplitz matrices. 
This way, matrix $\mathbf{H_{xy}}$ can be described by the \textit{block index} $q$ that represents the block diagonals of this matrix as a grid of $Q \times Q$ blocks $\mathbf{H}^{q}_\mathbf{xy}$, $q = -Q + 1, \dots, 0, \dots, Q - 1$:
\begin{equation}
\mathbf{H_{xy}} = \begin{bmatrix}
\mathbf{H}^{0}_\mathbf{xy}  & \mathbf{H}^{-1}_\mathbf{xy} & \cdots         & \mathbf{H}^{-Q+1}_\mathbf{xy} \\
\mathbf{H}^{1}_\mathbf{xy}  & \mathbf{H}^{0}_\mathbf{xy} & \ddots         & \vdots           \\ 
\vdots           & \ddots         & \ddots         & \mathbf{H}^{-1}_\mathbf{xy}   \\
\mathbf{H}^{Q-1}_\mathbf{xy} & \cdots         & \mathbf{H}^{1}_\mathbf{xy} & \mathbf{H}^{0}_\mathbf{xy}                
\end{bmatrix}_{N \times N} \: .
\label{eq:BTTB_Hxy}
\end{equation}
And each diagonal of the blocks are represented by $P \times P$ elements $h^{q}_{p}$, $p = -P + 1, \dots, 0, \dots, P - 1$:
\begin{equation}
\mathbf{H}^{q}_\mathbf{xy} =  \{h^{q}_p\} = \begin{bmatrix}
h^{q}_{0}   & h^{q}_{-1} & \cdots    & h^{q}_{-P+1} \\
h^{q}_{1}   & h^{q}_{0} & \ddots    & \vdots           \\ 
\vdots      & \ddots    & \ddots    & h^{q}_{-1}   \\
h^{q}_{P-1} & \cdots    & h^{q}_{1} & h^{q}_{0}                 
\end{bmatrix}_{P \times P} \: .
\label{eq:Hxy_block}
\end{equation}
In a $x$-\textit{oriented grid} $Q = N_{y}$, $P = N_{x}$ and $q$ and $p$ can be defined by the functions:
\begin{equation}
q(i, j) = \; l(i) - l(j) 
\label{eq:Hxy-q-x-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; k(i) - k(j) \quad ,
\label{eq:Hxy-p-x-oriented}
\end{equation}
where $l(i)$ and $l(j)$ are defined by equation \ref{eq:l-x-oriented} 
and $k(i)$ and $k(j)$ are defined by equation \ref{eq:k-x-oriented}.
For $y$-oriented grids, $Q = N_{x}$, $P = N_{y}$ and the block indices
$q$ and $p$ are defined, respectively, by the following integer functions 
of the matrix indices $i$ and $j$:
\begin{equation}
q(i, j) = \;  k(i) - k(j)  
\label{eq:Hxy-q-y-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \;  l(i) - l(j)  \quad ,
\label{eq:Hxy-p-y-oriented}
\end{equation}
Important to clarify that in this case, as a skew symmetric matrix, the values of oposing diagonals have oposing signals, e.g., $\mathbf{H}^{-1}_\mathbf{xy} = -\mathbf{H}^{1}_\mathbf{xy}$ and $h^{q}_{-1} = -h^{q}_{1} $.

%======================================================================================
%\subsection{Structure of the components matrices $\mathbf{H_{xz}}$}
%======================================================================================

By substituting equations \ref{eq:xi} and \ref{eq:yi} in equation \ref{eq:Hi}, the elements of $\mathbf{H_{xz}}$, are given by:
\begin{equation}
h^{xz}_{ij} = \frac{3 (\Delta k_{ij} \, \Delta x)(\Delta z)}{\left[ 
	\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
	\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
	\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: ,
\label{eq:hxz_mag}
\end{equation}

The component $\mathbf{H_{xz}}$ (equation \ref{eq:hxz_mag}) of matrix $\mathbf{H}$ (equation \ref{eq:Hi}) are skew symmetric-Block-Toeplitz symmetric-Toeplitz-Block matrices. This means that $\mathbf{H_{xz}}$ is Toeplitz and skew symmetric by its blocks and each of the blocks are symmetric Toeplitz matrices. 
Thus, matrix $\mathbf{H_{xz}}$ can be described by the \textit{block index} $q$ that represents the block diagonals of this matrix as a grid of $Q \times Q$ blocks $\mathbf{H}^{q}_\mathbf{xz}$, $q = -Q + 1, \dots, 0, \dots, Q - 1$:
\begin{equation}
\mathbf{H_{xz}} = \begin{bmatrix}
\mathbf{H}^{0}_\mathbf{xz}  & \mathbf{H}^{-1}_\mathbf{xz} & \cdots         & \mathbf{H}^{-Q+1}_\mathbf{xz} \\
\mathbf{H}^{1}_\mathbf{xz}  & \mathbf{H}^{0}_\mathbf{xz} & \ddots         & \vdots           \\ 
\vdots           & \ddots         & \ddots         & \mathbf{H}^{-1}_\mathbf{xz}   \\
\mathbf{H}^{Q-1}_\mathbf{xz} & \cdots         & \mathbf{H}^{1}_\mathbf{xz} & \mathbf{H}^{0}_\mathbf{xz}                
\end{bmatrix}_{N \times N} \: .
\label{eq:BTTB_Hxz}
\end{equation}
And each diagonal of the blocks are represented by $P \times P$ elements $h^{q}_{p}$, $p = 0, \dots, P - 1$:
\begin{equation}
\mathbf{H}^{q}_\mathbf{xz} =  \{h^{q}_p\} = \begin{bmatrix}
h^{q}_{0}   & h^{q}_{1} & \cdots    & h^{q}_{P-1} \\
h^{q}_{1}   & h^{q}_{0} & \ddots    & \vdots           \\ 
\vdots      & \ddots    & \ddots    & h^{q}_{1}   \\
h^{q}_{P-1} & \cdots    & h^{q}_{1} & h^{q}_{0}                 
\end{bmatrix}_{P \times P} \: .
\label{eq:Hxz_block}
\end{equation}
In a $x$-\textit{oriented grid} $Q = N_{y}$, $P = N_{x}$ and $q$ and $p$ can be defined by the functions:
\begin{equation}
q(i, j) = \; l(i) - l(j) 
\label{eq:Hxz-q-x-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; \mid k(i) - k(j) \mid \quad ,
\label{eq:Hxz-p-x-oriented}
\end{equation}
where $l(i)$ and $l(j)$ are defined by equation \ref{eq:l-x-oriented} 
and $k(i)$ and $k(j)$ are defined by equation \ref{eq:k-x-oriented}.
For $y$-oriented grids, $Q = N_{x}$, $P = N_{y}$ and the block indices
$q$ and $p$ are defined, respectively, by the following integer functions 
of the matrix indices $i$ and $j$:
\begin{equation}
q(i, j) = \;  k(i) - k(j)  
\label{eq:Hxz-q-y-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; \mid l(i) - l(j) \mid \quad ,
\label{eq:Hxz-p-y-oriented}
\end{equation}
In this case as a skew symmetric matrix by blocks, the values of oposing diagonals blocks have oposing signals, e.g., $\mathbf{H}^{-1}_\mathbf{xz} = -\mathbf{H}^{1}_\mathbf{xz}$ but each block is a symmetric matrix.

%======================================================================================
%\subsection{Structure of the components matrices $\mathbf{H_{yz}}$}
%======================================================================================

Finally, by substituting equations \ref{eq:xi} and \ref{eq:yi} in equation \ref{eq:Hi}, we can also describe the elements of $\mathbf{H_{yz}}$, as:
\begin{equation}
h^{yz}_{ij} = \frac{3 (\Delta l_{ij} \, \Delta y )(\Delta z)}{\left[ 
	\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
	\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
	\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: ,
\label{eq:hyz_mag}
\end{equation}

The component $\mathbf{H_{yz}}$ (equation \ref{eq:hyz_mag}) of matrix $\mathbf{H}$ (equation \ref{eq:Hi}) are symmetric-Block-Toeplitz skew symmetric-Toeplitz-Block matrices. This means that $\mathbf{H_{yz}}$ is Toeplitz and symmetric by its blocks and each of the blocks are skew symmetric Toeplitz matrices.
Thus, matrix $\mathbf{H_{yz}}$ can be described by the \textit{block index} $q$ that represents the block diagonals of this matrix as a grid of $Q \times Q$ blocks $\mathbf{H}^{q}_\mathbf{yz}$, $q = 0, \dots, Q - 1$:
\begin{equation}
\mathbf{H_{yz}} = \begin{bmatrix}
\mathbf{H}^{0}_\mathbf{yz}  & \mathbf{H}^{1}_\mathbf{yz} & \cdots         & \mathbf{H}^{Q-1}_\mathbf{yz} \\
\mathbf{H}^{1}_\mathbf{yz}  & \mathbf{H}^{0}_\mathbf{yz} & \ddots         & \vdots           \\ 
\vdots           & \ddots         & \ddots         & \mathbf{H}^{1}_\mathbf{yz}   \\
\mathbf{H}^{Q-1}_\mathbf{yz} & \cdots         & \mathbf{H}^{1}_\mathbf{yz} & \mathbf{H}^{0}_\mathbf{yz}                
\end{bmatrix}_{N \times N} \: .
\label{eq:BTTB_Hyz}
\end{equation}
And each diagonal of the blocks are represented by $P \times P$ elements $h^{q}_{p}$, $p = -P + 1, \dots, 0, \dots, P - 1$:
\begin{equation}
\mathbf{H}^{q}_\mathbf{yz} =  \{h^{q}_p\} = \begin{bmatrix}
h^{q}_{0}   & h^{q}_{-1} & \cdots    & h^{q}_{-P+1} \\
h^{q}_{1}   & h^{q}_{0} & \ddots    & \vdots           \\ 
\vdots      & \ddots    & \ddots    & h^{q}_{-1}   \\
h^{q}_{P-1} & \cdots    & h^{q}_{1} & h^{q}_{0}                 
\end{bmatrix}_{P \times P} \: .
\label{eq:Hyz_block}
\end{equation}
In a $x$-\textit{oriented grid} $Q = N_{y}$, $P = N_{x}$ and $q$ and $p$ can be defined by the functions:
\begin{equation}
q(i, j) = \; \mid l(i) - l(j) \mid
\label{eq:Hyz-q-x-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; k(i) - k(j) \quad ,
\label{eq:Hyz-p-x-oriented}
\end{equation}
where $l(i)$ and $l(j)$ are defined by equation \ref{eq:l-x-oriented} 
and $k(i)$ and $k(j)$ are defined by equation \ref{eq:k-x-oriented}.
For $y$-oriented grids, $Q = N_{x}$, $P = N_{y}$ and the block indices
$q$ and $p$ are defined, respectively, by the following integer functions 
of the matrix indices $i$ and $j$:
\begin{equation}
q(i, j) = \; \mid k(i) - k(j) \mid
\label{eq:Hyz-q-y-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; l(i) - l(j) \quad ,
\label{eq:Hyz-p-y-oriented}
\end{equation}
Being a symmetric matrix by blocks, the values of $\mathbf{H_{yz}}$ from oposing diagonals blocks are equal, but each block have skew symmetric oposing diagonals, i.e., $h^{q}_{-1} = - h^{q}_{1}$.

In general, matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) is a non-symmetric BTTB, i.e., its blocks are non-symmetric ($\mathbf{A}^{-Q+1} \neq \mathbf{A}^{Q-1} $) and its elements also are non-symmetric ($a^{q}_{-1} \neq a^{q}_{1}$). Depending on specific values of the main field direction and the equivalent sources magnetization directions, matrix $\mathbf{A}$ can assume other structures, for example, when $\hat{\mathbf{F}} = [0, 0, 1]$ and $\hat{\mathbf{u}} = [0, 0, 1]$ it becomes symmetric. In this work, we are considering the more commom situation for the matrix $\mathbf{A}$.

Also differently for the symmetric sensitivity matrix described by \cite{takahashi2020convolutional}, the non-symmetric BTTB matrix cannot be reconstructed only by its first column. The construction of the matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) needs four columns: the first and last columns of the first column of blocks and the first and last columns of the last column of blocks. This has a physical implication in the equivalent layer which is not possible to use only one equivalent source to reprduce all elements of matrix $\mathbf{A}$, such as in the gravity case as demonstrared by \cite{takahashi2020convolutional}. Rather, in the magnetic case it takes four equivalent sources positioned at each corner of the equivalent layer. Figure \ref{fig:4_equivalent_sources} shows the positioning of the equivalent sources in a regular grid $N_x = 4$
and $N_y = 3$ necessary to calculate the four columns capable of recover the matrix $\mathbf{A}$.

In this work, we propose a different approach, by calculating the first column of all six different components of second derivatives matrices from $\mathbf{H}_{ij}$ (equation \ref{eq:Hi}). These matrices are, in fact, symmetrics or skew-symmetrics BTTBs, meaning that the first column has all elements of each matrix.

%======================================================================================
\subsection{Standard Conjugate Gradient Least Squares (CGLS) method}
%======================================================================================

The computational cost associated with the classical method for estimating the parameter 
vector $\mathbf{p}$ by solving the linear system \ref{eq:normal-equations} can be very high 
or ever prohibitive when dealing with large data sets. In these cases, a well-known alternative
is solving the normal equations (equation \ref{eq:normal-equations}) iteratively by 
using the standard Conjugate Gradient Least Squares (CGLS) method. Below we present a pseudocode 
for the standard CGLS method:

\begin{algorithm}[H]
	Input: $\mathbf{A}$ and $\mathbf{d}^{o}$.
	
	Output: Estimated parameter vector $\tilde{\mathbf{p}}$.
	
	Set $it = 0$, $\tilde{\mathbf{p}}_{(it)} = \mathbf{0}$, $\mathbf{c}_{(it-1)} = \mathbf{0}$, $\beta_{(it)} = 0$, $\mathbf{s}_{(it)} = \mathbf{d}^{o}$ and $\mathbf{r}_{(it)} = \mathbf{A}^{\top} \mathbf{s}_{(it)}$.
	
	1 - If $it > 0$, $\beta_{(it)} = \dfrac{\| \mathbf{r}_{(it)} \|_{2}^{2}}{\| \mathbf{r}_{(it - 1)} \|_{2}^{2}}$
	
	2 - $\mathbf{c}_{(it)} = \mathbf{r}_{(it)} + \beta_{(it)} \, \mathbf{c}_{(it - 1)}$
	
	3 - $\alpha_{(it)} = \dfrac{{\| \mathbf{r}_{(it)}\|_{2}^{2}}}{\| \mathbf{A} \, \mathbf{c}_{(it)} \|_{2}^{2}}$
	
	4 - $\tilde{\mathbf{p}}_{(it + 1)} = \tilde{\mathbf{p}}_{(it)} + \alpha_{(it)} \, \mathbf{c}_{(it)}$
	
	5 - $\mathbf{s}_{(it + 1)} = \mathbf{s}_{(it)} - \alpha_{(it)} \, \mathbf{A} \, \mathbf{c}_{(it)}$
	
	6 - $\mathbf{r}_{(it + 1)} = \mathbf{A}^{\top} \, \mathbf{s}_{(it + 1)}$
	
	7 - $it = it + 1$
	
	8 - Repeat previous steps until convergence.
	
	\caption{Standard CGLS pseudocode \citep[][ p. 166]{aster2019parameter}.}
\label{al:std-cgls-algorithm}
\end{algorithm}

Setting a convergence criteria based on the minimum tolerance of the residuals is a good 
option to carry out this algorithm efficiently and still obtaining very good results. 
Another possibility is to set an invariance to the Euclidean norm of residuals between 
iterations, which would increase algorithm runtime, but with smaller residuals. 
We chose the first option, as we achieve satisfactory results. 
The estimated parameter vector obtained by using the standard CGLS method 
(Algorithm \ref{al:std-cgls-algorithm}) will be referenced throughout this work as 
the \textit{standard CGLS solution}.

Note that the standard CGLS solution (Algorithm \ref{al:std-cgls-algorithm}) requires 
neither inverse matrix nor matrix-matrix product. Instead, it only requires: one matrix-vector 
product out of the loop and two matrix-vector products per iteration (in steps 3 and 6). 
In the following subsections, we will show how to compute these three matrix-vector products 
efficiently by exploring the structure of the sensitivity matrix $\mathbf{A}$.

PAREI AQUI

%======================================================================================
\subsection{CGLS matrix-vector substitution}
%======================================================================================

As pointed earlier in this work, the main improvement inside the CGLS method (Algorithm \ref{al:std-cgls-algorithm}) for estimating the parameter vector $\hat{\mathbf{p}}$ (equation \ref{eq:estimated-p-parameter-space}) is to substitute the matrix-vector multiplication $\mathbf{A}^{\top} \mathbf{s}^{(0)}$ out of the loop and the two matrix-vector multiplications inside the loop at steps 3 an 6, $\mathbf{A} \, \mathbf{c}^{(it)}$ and $\mathbf{A}^{\top} \, \mathbf{s}^{(it + 1)}$, that is necessary at each iteration and takes most of its runtime.

Our method consists in calculating the six first columns of the second derivatives of $\mathbf{H}$ (equation \ref{eq:Hi}) and embedding them into the first six columns of the block-circulant circulant-block (BCCB) matrices related to the $\mathbf{H}$ components. Thus, it is possible to calculate the first column of the BCCB matrix embbeded from matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) by multiplying each component with its respective constants and summing as shown in equation \ref{eq:aij_mag_expand}. In \cite{takahashi2020convolutional}, Appendix A, the authors demonstrated in details how to transform a symmetric BTTB matrix into a BCCB matrix $\mathbf{C}$. The process here is the same and that work can be referenced to achieve the same results.

A new auxuliary linear system is constructed to carry the matrix-vector product:
\begin{equation}
\mathbf{w} = \mathbf{C} \mathbf{v} \: ,
\label{eq:w_Cv}
\end{equation}
where
\begin{equation}
\mathbf{w} = \begin{bmatrix}
\mathbf{w}_{0} \\
\vdots \\
\mathbf{w}_{Q - 1} \\
\mathbf{0}_{2N \times 1}
\end{bmatrix}_{4N \times 1} \quad ,
\label{eq:w-vector}
\end{equation}
\begin{equation}
\mathbf{w}_{q} = \begin{bmatrix}
\mathbf{d}_{q}(\mathbf{p}) \\
\mathbf{0}_{P \times 1}
\end{bmatrix}_{2P \times 1}
\label{eq:wq-vector} \quad ,
\end{equation}
\begin{equation}
\mathbf{v} = \begin{bmatrix}
\mathbf{v}_{0} \\
\vdots \\
\mathbf{v}_{Q - 1} \\
\mathbf{0}_{2N \times 1}
\end{bmatrix}_{4N \times 1} \quad ,
\label{eq:v-vector}
\end{equation}
and
\begin{equation}
\mathbf{v}_{q} = \begin{bmatrix}
\mathbf{p}_{q} \\
\mathbf{0}_{P \times 1}
\end{bmatrix}_{2P \times 1}
\label{eq:vq-vector} \quad ,
\end{equation}
where $\mathbf{C}$ (equation \ref{eq:w_Cv}) is a $4N \times 4N$ non-symmetric (BCCB) resulted from transforming $\mathbf{A}$ (equation \ref{eq:aij_mag}). Without having to calculate the whole BCCB matrix, its first column can be used to carry the multiplication of this new system (equation \ref{eq:w_Cv}). Appendix A and C in \cite{takahashi2020convolutional} shows how to use the 2D-FFT to compute the eigenvalues of matrix $\mathbf{C}$, store in a $2Q \times 2P$ matrix using the $vec$-operator and to carry the matrix-vector product. Denoting the matrix $\mathbf{L}$ as the eigenvalues matrix follows:
\begin{equation}
	\mathbf{F}_{2Q}^{\ast} \left[ 
	\mathbf{L} \circ \left(\mathbf{F}_{2Q} \, \mathbf{V} \, \mathbf{F}_{2P} \right) 
	\right] \mathbf{F}_{2P}^{\ast} = \mathbf{W} \: ,
	\label{eq:DFT-system}
\end{equation}
where the symbol ``$\circ$'' references the Hadamard product, i.e., a element-wise complex multiplication between the eingenvalues and the 2D-FFT of the matrix rearranged along the rows of the parameters $\mathbf{V}$ (equation \ref{eq:v-vector}) using the same $vec$-operator. The resulting inverse 2D-FFT denoted by $\mathbf{F}_{2Q}^{\ast}  \otimes \mathbf{F}_{2P}^{\ast}$ is also a $2Q \times 2P$ matrix ($\mathbf{W}$) that can be rearranged to the predicted data vector $\mathbf{d}(\hat{\mathbf{p}})$ size $N$.

%======================================================================================
\subsection{Computational performance}
%======================================================================================

To compare the efficiency of our algorithm we will use a numerical approach and calculate the floating-point operations (\emph{flops}), i.e., count the number of mathematical operations necessary to complete the estimative of parameter vector $\mathbf{\hat{p}}$ of the normal equations (equation \ref{eq:estimated-p-parameter-space}) and both the CGLS methods (algorithm \ref{al:std-cgls-algorithm}) for calculating the matrix-vector product by its standart way and our approach.

The \emph{flops} needed to solve the linear system in equation \ref{eq:estimated-p-parameter-space} using the Cholesky factorization is:
\begin{equation}
f_{classical} =  \dfrac{7}{3} N^{3} + 6 N^{2}\: ,
\label{eq:flops-normal-cholesky}
\end{equation}
where $N$ is the total number of observation points and also the size of estimated parameter vector $\mathbf{\hat{p}}$.

For the more efficient CGLS algorithm the estimative can be done in:
\begin{equation}
f_{cgls} =  2 N^{2} + it \, (4 N^{2} + 12 N) \: .
\label{eq:flops-cgls}
\end{equation}
However, our approach reduces further to:
\begin{equation}
f_{ours} =  \kappa  \, 16 N \log_2(4 N) + 24 N + it \, (\kappa  \, 16 N \log_2 (4 N) + 60 N) \: ,
\label{eq:flops-cgls-bccb}
\end{equation}
where $\kappa$ depends on the FFT algorithm. By default, in this work we will use $\kappa = 5$ for the \emph{radix-2} algorithm \citep{vanloan1992}.

Figure \ref{fig:flops} shows a comparative between the methods varying the number of observation points up to $1,000,000$, where it is possible to observe a reduction of $10^7$ orders of magnitude to estimate parameter vector $\mathbf{\hat{p}}$ in relation to the non-iterative classical method and $10^3$ orders of magnitude in relation to the standart CGLS algorithm using $50$ iterations. A more detailed, step by step, flops count of the classical and CGLS algorithm can be found in Appendix A.

In Figure \ref{fig:solve_time} we show the time necessary to construct matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) and solve the linear system up to $10,000$ points of observation. With this dataset the classical method takes more than sixty-three seconds, the CGLS more than twelve seconds, while our method takes only half a second. The cpu used for this test was a intel core i7-7700HQ@2.8GHz.

In Figure \ref{fig:sources_time} a comparison between the time to complete the task to calculate the first column of the BCCB matrix embbeded from the from $\mathbf{A}$ (equation \ref{eq:aij_mag}) by using only one equivalent source, i.e., calculating all six first column of the second derivatives matrices from $\mathbf{H}$ (equation \ref{eq:Hi}) and using four equivalent sources to calculate the four necessary columns from the non-symmetric matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}). Although, very similar in time, with one source a small advantage can be observed as the number of data $N$ increases and goes beyond $N = 200,000$. This test was done from $N = 10,000$ to $N = 700,000$ with increases of $5,625$ observation points.

In Table \ref{tab:RAM-usage} there is comparison between how much RAM memory is adressed to store the sensitivity matrix for each of the methods. The classical approach and the CGLS have to store the whole matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}), this means that a dataset  with for example $N = 10,000$ observation points, the sensitivity matrix has $N^2 = 100,000,000$ elements and takes approximately $763$ Megabytes of memory (8 bytes per element). For our method, it is necessary to store the first six columns of each of the components from matrix $\mathbf{H}$ (equation \ref{eq:Hi}) embedded into the BCCB matrices. With the same dataset $N = 10,000$ it needs $1.831$ Megabytes. After completing the steps to store the eigenvalues of matrix $\mathbf{C}$ (equation \ref{eq:w_Cv}) it takes only $0.6104$ Megabytes. Here, we are considering 16 bytes per element as the eigenvalues are complex numbers resulting from the 2D FFT. For a bigger dataset as $N = 1,000,000$ the amount of RAM necessary goes to $7,629,395$, $183.096$ and $61.035$ Megabytes, respectively, showing the necessity to find improved and efficient methods for the equivalent layer technique as the one presented in this work. We remember that throughout our work we are always considering $N = M$.









